% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sits_torch_optim_yogi.R
\name{optim_yogi}
\alias{optim_yogi}
\title{Yogi optimizer}
\usage{
optim_yogi()
}
\arguments{
\item{params}{List of parameters to optimize.}

\item{lr}{Learning rate (default: 1e-3)}

\item{betas}{Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))}

\item{eps}{Term added to the denominator to improve numerical stability
(default: 1e-8)}

\item{initial_accumulator}{Initial values for first and
second moments.}

\item{weight_decay}{Weight decay (L2 penalty) (default: 0)}
}
\value{
An optimizer object implementing the `step` and `zero_grad` methods.
}
\description{
R implementation of the Yogi optimizer proposed
by Zaheer et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.

From the abstract by the paper by Zaheer et al.(2019):
Adaptive gradient methods that rely on scaling gradients
down by the square root of exponential moving averages
of past squared gradients, such RMSProp, Adam, Adadelta have
found wide application in optimizing the nonconvex problems
that arise in deep learning. However, it has been recently
demonstrated that such methods can fail to converge even
in simple convex optimization settings.
Yogi is a new adaptive optimization algorithm,
which controls the increase in effective learning rate,
leading to even better performance with similar theoretical
guarantees on convergence. Extensive experiments show that
Yogi with very little hyperparameter tuning outperforms
methods such as Adam in several challenging machine learning tasks.
}
\references{
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar,
"Adaptive Methods for Nonconvex Optimization",
Advances in Neural Information Processing Systems 31 (NeurIPS 2018).
https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization
}
\author{
Gilberto Camara, \email{gilberto.camara@inpe.br}

Rolf Simoes, \email{rolf.simoes@inpe.br}

Felipe Souza, \email{lipecaso@gmail.com}

Alber Sanchez, \email{alber.ipia@inpe.br}
}
