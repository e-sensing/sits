% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sits_tuning.R
\name{sits_tuning}
\alias{sits_tuning}
\title{Tuning deep learning models hyper-parameters}
\usage{
sits_tuning(
  samples,
  samples_validation = NULL,
  validation_split = 0.2,
  ml_method = sits_tempcnn(),
  params = list(optimizer = torchopt::optim_adamw, opt_hparams = list(lr = beta(0.3,
    5))),
  trials = 30,
  multicores = 2,
  progress = FALSE
)
}
\arguments{
\item{samples}{Time series set to be validated.}

\item{samples_validation}{Time series set used for validation.}

\item{validation_split}{Percent of original time series set to be used
for validation (if samples_validation is NULL)}

\item{ml_method}{Machine learning method.}

\item{params}{List with hyper parameters to be passed to
\code{ml_method}. User can use \code{uniform}, \code{choice},
\code{randint}, \code{normal}, \code{lognormal}, \code{loguniform},
and \code{beta} distribution functions to randomize parameters.}

\item{trials}{Number of random trials to perform the random search.}

\item{multicores}{Number of cores to process in parallel}

\item{progress}{Show progress bar?}
}
\value{
A list containing the best model and a tibble with all performances
}
\description{
Deep learning models use stochastic gradient descent (SGD) techniques to
find optimal solutions. To perform SGD, models use optimization
algorithms which have hyperparameters that have to be adjusted
to achieve best performance for each application.

This function performs a random search on values of selected hyperparameters.
Instead of performing an exhaustive test of all parameter combinations,
it selecting them randomly. Validation is done using an independent set
of samples or by a validation split.  The function returns the
best hyper-parameters in a list.
}
\note{
Please refer to the sits documentation available in
<https://e-sensing.github.io/sitsbook/> for detailed examples.
}
\references{
James Bergstra, Yoshua Bengio,
 "Random Search for Hyper-Parameter Optimization".
 Journal of Machine Learning Research. 13: 281â€“305, 2012.
}
