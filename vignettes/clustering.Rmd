---
output:
    pdf_document:
      citation_package: natbib
      df_print: tibble
      fig_caption: yes
      keep_tex: no
      template: "../inst/extdata/markdown/latex-ms.tex"
title: 'Clustering of Satellite Image Time Series with SITS'
author:
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Lorena Santos
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Karine Ferreira
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Rolf Simoes
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Gilberto Camara
date: "`r format(Sys.time(), '%B %d, %Y')`"
endnote: false
fontfamily: mathdesign
fontfamilyoptions: adobe-utopia
fontsize: 11pt
graphics: true
mathtools: true
bibliography: ../inst/extdata/markdown/references-sits.bib
abstract: One of the key challenges when using samples to train machine learning classification models is assessing their quality. Noisy and imperfect training samples can have a negative effect on classification performance.  Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. Representative samples lead to good classification maps. `sits` provides support for two clustering methods to test sample quality, which are agglomerative hierarchical clustering (AHC) and self-organizing maps (SOM).
vignette: |
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteIndexEntry{Clustering of Satellite Image Time Series with SITS}
---
```{r, include = FALSE}
if (!requireNamespace("devtools", quietly = TRUE))
        install.packages("devtools")
devtools::load_all(".")
library(sits)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction: Clustering  for sample quality control

One of the key challenges when using samples to train machine learning classification models is assessing their quality. Noisy and imperfect training samples can have a negative effect on classification performance [@Frenay2013].  Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. Representative samples lead to good classification maps. `sits` provides support for two clustering methods to test sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b)  Self-organizing Maps (SOM).

# Hierachical clustering

Cluster analysis has been used for many purposes in satellite image time series literature ranging from unsupervised classification [@Petitjean2011], and pattern detection [@Romani2011]. Here, we are interested in the second use of clustering, using it as a way to improve training data to feed machine learning classification models. In this regard, cluster analysis can assist the identification of structural *time series patterns* and anomalous samples [@Romani2011], [@Chandola2009]. 

Agglomerative hierarchical clustering (AHC) is a family of methods that groups elements using a distance function to associate a real value to a pair of elements. From this distance measure, we can compute the dissimilarity between any two elements from a data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. AHC approach is suitable for the purposes of samples data exploration due to its visualization power and ease of use [@Keogh2003]. Moreover, AHC does not require a predefined number of clusters as an initial parameter. This is an important feature in satellite image time series clustering since defining the number of clusters present in a set of multi-attribute time series is not straightforward [@Aghabozorgi2015].

The main result of AHC method is a *dendrogram*. It is the ultrametric relation formed by the successive merges in the hierarchical process that can be represented by a tree. Dendrograms are quite useful to decide the number of clusters to partition the data. It shows the height where each merging happens, which corresponds to the minimum distance between two clusters defined by a *linkage criterion*. The most common linkage criteria are: *single-linkage*, *complete-linkage*, *average-linkage*, and *Ward-linkage*. Complete-linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, which can increase the resulting intracluster data variance. As an alternative, Ward proposes a criteria to minimize the data variance by means of either *sum-of-squares* or *sum-of-squares-error* [@Ward1963]. Ward's intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape [@Hennig2015]. In `sits`, a dendrogram can be generated by `sits_dendrogram()`. The following codes illustrate how to create, visualize, and cut a dendrogram (for details, see `?sits_dendrogram()`).


After creating a dendrogram, an important question emerges: *where to cut the dendrogram?* The answer depends on what are the purposes of the cluster analysis [@Hennig2015]. If one is interested in an unsupervised classification, it is common to use *internal validity indices*, such as silhouettes [@Rousseeuw1987], to help determine the best number of clusters. However, if one is interested in understanding the structure of a labeled data set, or in the identifying sample anomalies, as we are here, one can use *external validity indices* to assist the semi-supervised procedure in order to achieve the optimal correspondence between clusters and classes partitions. In this regard, we need to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to their known classes. To help this process, `sits` provides `sits_dendro_bestcut()` function that computes an external validity index *Adjusted Rand Index* (ARI) for a series of different number of generated clusters. This function returns the height where the cut of the dendrogram maximizes the index.

In this example, the height optimizes the ARI and generates $6$ clusters. The ARI considers any pair of distinct samples and computes the following counts:
(a) the number of distinct pairs whose samples have the same label and are in the same cluster;
(b) the number of distinct pairs whose samples have the same label and are in different clusters;
(c) the number of distinct pairs whose samples have different labels and are in the same cluster; and
(d) the number of distinct pairs whose samples have the different labels and are in different clusters.
Here, $a$ and $d$ consist in all agreements, and $b$ and $c$ all disagreements. The ARI is obtained by:

$$
ARI=\frac{a+d-E}{a+d+b+c-E},
$$
where $E$ is the expected agreement, a random chance correction calculated by 
$$
E=(a+b)(b+c)+(c+d)(b+d).
$$ 

Unlike other validity indexes such as Jaccard (${J=a/(a+b+c)}$), Fowlkes-Mallows (${FM=a/(a^2+a(b+c)+bc)^{1/2}}$), and Rand (the same as ARI without the $E$ adjustment) indices, ARI is more appropriate either when the number of clusters is outweighed by the number of labels (and *vice versa*) or when the amount of samples in labels and clusters is imbalanced [@Hubert1985], which is usually the case.

```{r dendrogram, cache=TRUE, fig.align="center", fig.height=4.1, fig.width=5}
# take a set of patterns for 2 classes
# create a dendrogram object, plot, and get the optimal cluster based on ARI index
clusters.tb <- sits::sits_cluster_dendro(cerrado_2classes, bands = c("ndvi", "evi"))

# show clusters samples frequency
sits::sits_cluster_frequency(clusters.tb)
```

Note in this example that almost all clusters has a predominance of either "Cerrado" or "Pasture" classes with the exception of cluster $3$. The contingency table plotted by `sits_cluster_frequency()` shows how the samples are distributed across the clusters and helps to identify two kinds of confusions. The first is relative to those small amount of samples in clusters dominated by another class (*e.g.* clusters $1$, $2$, $4$, $5$, and $6$), while the second is relative to those samples in non-dominated clusters (*e.g.* cluster $3$). These confusions can be an indication of samples with poor quality, an inadequacy of selected parameters for cluster analysis, or even a natural confusion due to the inherent variability of the land classes.

The result of the `sits_cluster` operation is a `sits_tibble` with one additional column, called "cluster". Thus, it is possible to remove clusters with mixed classes using standard `R` such as those in the `dplyr` package. In the example above, removing cluster $3$ can be done using the `dplyr::filter` function.

```{r}
# remove cluster 3 from the samples
clusters_new.tb <- dplyr::filter(clusters.tb, cluster != 3)

# show new clusters samples frequency
sits_cluster_frequency(clusters_new.tb)
```

The resulting clusters still contained mixed labels, possibly resulting from outliers. In this case, users may want to remove the outliers and leave only the most frequent class. To do this, one can use `sits_cluster_clean()`, which removes all minority samples, as shown below..

```{r}
# clear clusters, leaving only the majority class in each cluster
cleaned.tb <- sits_cluster_clean(clusters.tb)
# show clusters samples frequency
sits_cluster_frequency(cleaned.tb)
```

# Self-organizing maps

As an alternative for hierachical clustering for quality control of training samples, SITS provides a clustering technique based on self-organizing maps (SOM). SOM is a dimensionality reduction technique [@Kohonen2001], where high-dimensional data is mapped into two dimensions, keeping the topological relations between data patterns. The input data is a high dimset of tranining samples which are typically of a high dimension. For example, a time series of 25 instances of 4 spectral bands is a 100-dimensional data set. The general idea of SOM-based clustering is that, by projecting the high-dimensional data set of training samples into a 2D map, the units of the map (called "neurons") compete for each sample. It is expected that good quality samples of each class should be close together in the resulting map. 


The process of clustering  with SOM is done by `sits_cluster_som()` which creates a self-organizing map. First, a 2D grid of neurons is initialized randomly; each neuron is associated to a weight vector of the same dimension as the input space. For each time series sample, the algorithm finds the neuron with the smallest distance based on its weight vector. After the match, the neuron's weight vector and those of its neighbors are then updated. After all training samples are associated with neurons, each neuron is labeled using a majority vote, taking the most frequent class from the samples associated with it. In this way, SOM splits the output space into regions.  To increase the reliability of quality control procedures, the SOM clustering can be executed several times. Using this iterative procedure, the algorithm computes the probability that a sample belongs to each of the resulting clusters. From these probabilities, samples with similar phenological patterns are grouped together. This allows using SOM to detect and remove outliers; samples whose set of probabilities has a high variance are discarded.

More formally, for each neuron $j$ SOM uses a vector of weights, $w_j=[w_{j1},\ldots,w_{jn} ]$,that has the same dimension of the input vector of time series samples $x(t)=[x(t)_1,\ldots,x(t)_n ]$, where $n$ is the time series dimension. In the begining, the neurons are initialized randomly. At each training step, a time series sample $x(t)$ is presented to the network in order to find the neuron whose weight vector has the smaller distance to the sample. Distance metrics $D_{j}$ compute the distance between input sample $x(t)$ and the weight vector of a neuron. This is computed for each neuron $j$ in the output layer. The most commonly used metric is Euclidean distance, shown below.

$$
D_{j}=\sum_{i=1}^{n}{\sqrt{(x(t)_i{-}w_{ji})^{2}}}.
$$

The neuron that contains the shortest distance, defined here as ${b}$, is denoted as the best matching unit (BMU):

$$
 d_{b}= min \left\{D_1,\ldots, D_J \right\}.
$$

Once the BMU is selected for a given sample, its neighborhood must be updated. The weight of each neighbour is adjusted according to  the similarity with input vector. The equation for updating the weight vector is given by:

$$
 w_{j}(t{+}1)=  w_{j}(t){+}\alpha(t)* h_{b,j}(t) [x(t)_i{-}w_{j}(t)],
$$
where $\alpha(t)$ is the learning rate, which must be set as $0< \alpha(t) <1$, and $h_{b,j}(t)$ is a neighbourhood function.

As an example of the use of SOM clustering for quality control of samples, we take a data set 617 time series samples for the combination of the LANDSAT images for an area of the Brazilian Amazon rain forest. This area corresponds to the LANDAT WRS 226/04. The LANDSAT pixels have been combined with the MOD13Q1 collection 5 images, to fill the gaps where there is too much cloud in the LANDSAT data. The data set has the following classes (and samples per class): Deforestation_2014 (146 samples), Deforestation_2015 (198 samples), Forest (128 samples), and  Pasture (145 samples). We first run the SOM cluster function.

```{r}
# clustering time series using SOM
som_cluster <-
    sits::sits_som_map(
        prodes_226_064,
        grid_xdim = 10,
        grid_ydim = 10,
        alpha = 1.0,
        distance = "euclidean",
        iterations = 40
    )
```
The output of the `sits_som_map` is a list with 4 tibbles:

* the original time series with four additional columns: `id_sample` (the original id of each sample), `som_label` (the cluster label assigned to the respective neuron on the SOM map) `cluster_probability` (the probability of a samble belonging to that cluster) and `total_probability`. Each input sample can be assigned to two or more clusters with different probabilities. 

* one tibble with the association between the labels of original samples and the labels of the neurons.

* one tibble with the association bewteen each neuron and its neighbours.


To plot the kohonen map, use `plot.som_map()`, which can be called directly from `plot()`. The neurons are labelled using the majority voting.
```{r}
plot(som_cluster)
```

From the statistics obtained from function `sits_som_map`, a new dataset can be generated removing the samples with bad quality.
The function `sits_som_clean_samples()` removes the bad samples based on the probability a samples belongs to a cluster. 

```{r}
new_samples <- sits_som_clean_samples(som_cluster)
new_samples
```

To verify the quality of the clusters generated by SOM, a confusion matrix, the overall accuracy and the statistics about each class is evaluated.  
```{r}
cluster_overall <- sits_som_evaluate_cluster(som_cluster)
cluster_overall$confusion_matrix
```
Besides that, a table showing how a table showing how pure the cluster is computed.
```{r}
cluster_overall$mixture_cluster
```

Finally, a graphic showing the mixture within each cluster is presented.
```{r}
sits_som_plot_clusters(cluster_overall, "Confusion by cluster")
```
