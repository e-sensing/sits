---
output:
    pdf_document:
      citation_package: natbib
      df_print: tibble
      fig_caption: yes
      keep_tex: yes
      template: latex-ms.tex
title: "SITS: Data Analysis and Machine Learning using Satellite Image Time Series"
author:
- name: Rolf Simoes
  affiliation: National Institute for Space Research (INPE), Brazil
- name: Gilberto Camara
  affiliation: National Institute for Space Research (INPE), Brazil
- name: Alexandre Carvalho
  affiliation: Institute for Applied Economics Research (IPEA), Brazil
- name: Victor Maus
  affiliation: International Institute for Applied System Analysis (IIASA), Austria
- name: Gilberto Queiroz
  affiliation: National Institute for Space Research (INPE), Brazil
abstract: "Using time series derived from big Earth Observation data sets is one of the leading research trends in Land Use Science and Remote Sensing. One of the more promising uses of satellite time series is its application for classification of land use and land cover, since our growing demand for natural resources has caused major environmental impacts. Here, we present the open source R package for satellite image time series analysis, the `sits` package. The `sits` provides support on how to use statistical learning techniques with image time series. These methods include linear and quadratic discrimination analysis, support vector machines, random forests and neural networks."
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontfamily: mathdesign
fontfamilyoptions: adobe-utopia
fontsize: 11pt
bibliography: references-sits.bib
csl: plos-one.csl
endnote: false
graphics: true
mathtools: true
vignette: >
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteIndexEntry{SITS: Data Analysis and Machine Learning using Satellite Image Time Series}
---
```{r, include = FALSE}
library(sits)
library(tibble)
library(dtwclust)
```

# Introduction 

Earth observation satellites provide a continuous and consistent set of information about the Earth’s land and oceans. Most space agencies have adopted an open data policy, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a major challenge: *How to design and build technologies that allow the Earth observation community to analyse big data sets?*

The approach taken in the current work is to develop data analysis methods that work with satellite image time series. The time series are obtained by taking calibrated and comparable measures of the same location in Earth at different times. These measures can be obtained by a single sensor (*e.g.*, MODIS) or by combining different sensors (*e.g.*, LANDSAT-8 and SENTINEL-2). If obtained by frequent revisits, the temporal resolution of these data sets can capture the most important land use changes. 

Time series of remote sensing data show that land cover changes do not always occur in a progressive and gradual way, but they may also show periods of rapid and abrupt change followed either by a quick recovery [@Lambin2003]. Analyses of multiyear time series of land surface attributes, their fine-scale spatial pattern, and their seasonal evolution leads to a broader view of land-cover change. Satellite image time series have already been applied to applications such as mapping for detecting forest disturbance [@Kennedy2010], ecology dynamics [@Pasquarella2016], agricultural intensification [@Galford2008] and its impacts on deforestation [@Arvor2012].

In this paper, we present an open source R package for satellite image time series analysis `sits`. The `sits` package provides support on how to use statistical learning techniques with image time series. In a broad sense, statistical learning refers to a class of algorithms for classification and regression analysis [@Hastie2009]. These methods include linear and quadratic discrimination analysis, support vector machines, random forests and neural networks. In a typical classification problem, we have measures that capture class attributes. Based on these measures, referred as training data, one's task is to select a predictive model that allows inferring classes of a larger data set. 

In what follows, we describe the main characteristics of the `sits`. The first part describes the basic data structures used in it and the tools used for visualisation and data exploration. Then we show how to do data acquisition from external sources, with an emphasis on the WTSS (an acronym for Web Time Series Service) [@Ribeiro2015]. The next sections describe filtering and clustering techniques. We then discuss machine learning techniques for satellite image time series data and how to apply them to image time series. Finally, we present validation methods.

# Data Handling and Visualisation Basics in `sits`

The basic data unit in the `sits` package is the "`sits` tibble", which is a way of organizing a set of time series data with associated spatial information. In R, a `tibble` differs from the traditional data frame, insofar as a `tibble` can contain lists embedded as column arguments. Tibbles are part of the `tidyverse`, a collection of R package designed to work together in data manipulation. The `tidyverse` includes packages such as `ggplot2`, `dplyr` and `purrr` [@Wickham2017]. The `sits` makes extensive use of the `tidyverse`. 

For a better explanation of how the "`sits` tibble" works, we will read a data set containing 2,115 labelled samples of land cover in Mato Grosso state of Brazil. This state has 903,357 km^2^ of extension, being the third largest state of Brazil. It includes three of Brazil's biomes: Amazonia, Cerrado and Pantanal. It is the most important agricultural frontier of Brazil and is Brazil's largest producer of soybeans, corn and cotton. 

The samples contain time series extracted from the MODIS MOD13Q1 product from NASA from 2000 to 2016, provided every 16 days at 250-meter spatial resolution in the Sinusoidal projection. Based on ground surveys and high resolution imagery, we selected $2,115$ samples of nine classes: forest, cerrado, pasture, soybean-fallow, fallow-cotton, soybean-cotton, soybean-corn, soybean-millet, and soybean-sunflower.

```{r}
# data set of samples
# print the first three samples
samples_MT_9classes[1:3,]
```

The "`sits` tibble" contains data and metadata. The first six columns contain the metadata: spatial and temporal location, label assigned to the sample, and coverage from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the "WGS84" ellipsoid. For example, the first sample has been labelled "Pasture", at location (-55.1852, -10.8387), and is considered valid for the period (2013-09-14, 2014-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers involved in labelling the samples chose to use the agricultural calendar in Brazil, where the spring crop is planted in the months of September and October, and the autumn crop is planted in the months of February and March. For other applications and other countries, the relevant dates will most likely be different from those used in the example.

The "`sits` tibble" also contains the time series data for each spatiotemporal location. The timeseries data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. 

```{r}
# print the first 10 time series records of the first sample
samples_MT_9classes$time_series[[1]][1:3,]
```

The `sits` package provides functions for data manipulation and displaying information of a "`sits` tibble". For example, the command `sits_labels()` that shows the labels of the sample set and their frequencies.

```{r}
sits_labels(samples_MT_9classes)
```

In many cases, it is useful to relabel the data set. For example, there may be situations when one wants to use a smaller set of labels, since samples in one label on the original set may not be distinguishable from samples with other labels. We then should use `sits_relabel()`, which requires a conversion list (for details, see `?sits_relabel`).

Given that we have used the tibble data format for the metadata and and the embedded time series, one can use the functions of the `dplyr`, `tidyr` and `purrr` packages of the `tidyverse` [@Wickham2017] to process the data. For example, the following code uses the `sits_select()` function to get a subset of the sample data set with two bands ("ndvi" and "evi") and then uses the `dplyr::filter()` function to select the samples labelled either as "Cerrado" or "Pasture". We can then use the `sits_plot()` function to display the time series. Given a small number of samples to display, the `sits_plot()` function tries to group as many spatial locations together. In the following example, the first 15 samples of the "Cerrado" class all refer to the same spatial location in consecutive time periods. For this reason, these samples are plotted together.

```{r cerrado-15, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of the first 15 'Cerrado' samples from data set \\texttt{samples_MT_9classes} (different dates for the same point location)."}
# select the "ndvi" bands
samples_ndvi.tb <- 
    sits_select(samples_MT_9classes, 
                bands = c("ndvi"))
# select only the samples with the cerrado label
samples_cerrado.tb <- 
    dplyr::filter(samples_ndvi.tb, 
                  label == "Cerrado")
# plot the first 15 samples (different dates for the same points)
sits_plot(samples_cerrado.tb[1:15,])
```

For a large number of samples, where the amount of individual plots would be substantial, the default visualisation combines all samples together in a single temporal interval (even if they are valid for different years). Therefore, all samples of the same band and the same label are aligned to a common interval. This plot is useful to show the spread of values for the time series of each band. The strong red line in the plot shows the median of the values, and the two orange lines are the first and third interquartile ranges. The `sits_plot()` function has different ways of working. Please, refer to the package documentation for more details.

```{r cerrado-all, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of all 'Cerrado' samples from data set \\texttt{samples_MT_9classes}."}
# plot all cerrado samples together (shows the distribution)
sits_plot(samples_cerrado.tb)
```

Usually, samples are provided by experts whose take *in-loco* field observations or recognises land classes through high resolution images. In any case, we need access to a data source to fetch time series data regarding a spatiotemporal location of interest. The process of importing data samples is discussed in the next section.

# Importing Data into `sits`

The `sits` package allows different methods of data input, including: (a) obtain data from a WTSS (Web Series Time Service); (b) obtain data from the SATVEG service developed by EMBRAPA (Brazil's Agriculture Research Agency). (c) read data stored in a time series in the ZOO format [@Zeileis2005]; (d) read a time series from a RasterBrick [@Hijmans2015]. Option (d) will be described in the section were we describe raster processing. The WTSS service is a light-weight service, designed to retrieve time series for selected locations and periods [@Vinhas2016], been implemented by the research team of the National Institute for Space Research to allow remote access to time series data. The SATVEG service provides NDVI and EVI time series vegetation indices from MODIS image from whole Brazilian territory [@Embrapa2014]. To view service details, the user needs to call `sits_services()` that provides information on the coverages available on the server.

After finding out which coverages are available at the different time series services, one may request specific information on each coverage by using  `sits_coverage()`. This lists the contents of the data set, including source, bands, spatial extent and resolution, time range, and temporal resolution. This information is then stored in a tibble for later use.

```{r}
# get information about a specific coverage from WTSS
coverage.tb <- 
    sits_coverage(service  = "WTSS", 
                  product  = "MOD13Q1", 
                  coverage = "mod13q1_512")
coverage.tb[, c("xmin","xmax","ymin","ymax",
                "start_date", "end_date")]
```

The user can request one or more time series points using `sits_getdata()`. This function provides a general means of access to image time series. In its simplest fashion, the user provides the latitude and longitude of the desired location, the product and coverage names, the bands, and the start date and end date of the time series. If the start and end dates are not provided, all available period is retrived. The result is a tibble that can be visualised using `sits_plot()`.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="NDVI and EVI time series fetched from WTSS service."}
# a point in the transition forest pasture in Northern MT
# obtain a time series from the WTSS server for this point
series.tb <- 
    sits_getdata(longitude  = -55.57320, 
                 latitude   = -11.50566, 
                 service    = "WTSS", 
                 product    = "MOD13Q1",
                 coverage   = "mod13q1_512", 
                 bands      = c("ndvi", "evi"),
                 start_date = "2001-01-01", 
                 end_date   = "2016-12-31")
# plot the series
sits_plot(series.tb)
```

A useful case is when users have a set of labelled samples, that are to be used as a training data set. In this case, one usually has trusted observations which are labelled and commonly stored in plain text CSV files. The `sits_getdata()` function can receive a CSV file path as an argument. The CSV file must provide for each time series, its latitude and longitude, the start and end dates, and a label associated to a ground sample. 

After importing the samples time series, it is useful to explore the data and see how is it underlying structured and its inter-class separability. For example, We can note in the figure above the variability of 400 time series samples along time. Those samples were collected from different years and/or locations. The scattering behaviour is intrinsic to remote sensing data. Atmospheric noise, sun angle, interferences on observations or different equipaments specifications, as well as the very nature of the climate-land dynamics can be sources of such variability [@Atkinson2012]. One helpful technique to explore such properties is *cluster analysis*. In the following section we present a cluster technique supported by `sits`. 

# Clustering in satellite image time series

Cluster analysis has been used for many purposes in satellite image time series literature ranging from unsupervised classification [@Petitjean2011], and pattern detection [@Romani2011]. Here, we are interested in the second use of clustering, as a way to improve training data to use in machine learning classification models. In this regard, cluster analysis can assist the identification of structural *time series patterns*, and anomalous samples [@Romani2011], [@Chandola2009]. `sits` provides support for the agglomerative hierarchical clustering (AHC). 

Hierarchical clustering is a family of methods that groups elements using a distance function to associate a real value to a pair of elements. From this distance measure, we can compute the dissimilarity between any two elements from the data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. The AHC approach is suitable for the purposes of samples data exploration awe its visualisation power and ease of use [@Keogh2003]. Moreover, AHC does not require a predefined number of clusters as an initial parameter. This is an important feature in satellite image time series clustering since it is not easy to define the number of clusters present in a set of multi-attribute time series [@Aghabozorgi2015].

The main result of AHC method is the *dendrogram*. A *dendrogram* is the ultrametric relation formed by the successive merges in the hierarchical process that can be represented by a tree. Dendrograms are quite useful to decide on the number of clusters has the data. It shows the height where each merging happened, which corresponds to the minimum distance between two clusters defined by a *linkage criterion*. The most commom linkage criteria are: *single-linkage*, *complete-linkage*, *average-linkage*, and *Ward-linkage*. Complete-linkage prioritises the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, that can increase the resulting intracluster data variance. As an alternative, Ward proposes a criteria to minimise the data variance by means of either *sum-of-squares* or *sum-of-squares-error* [@Ward1963]. Ward's intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape [@Hennig2015]. In `sits`, a dendrogram can be generated by `sits_dendrogram()`. The following codes illustrate how to create, visualise, and cut a dendrogram (for details, see `?sits_dendrogram()`).

```{r dendrogram, cache=TRUE, fig.align="center", fig.height=4.1, fig.width=5}
# take a set of patterns for 2 classes
# create a dendrogram object with default clustering parameters
dendro <- sits_dendrogram(cerrado_2classes)
# plot the resulting dendrogram
sits_plot_dendrogram(cerrado_2classes, 
                     dendro)
```

After the creation of a dendrogram, an important question emerges: *where to cut the dendrogram?* The answer depends on what are the purposes of the cluster analysis [@Hennig2015]. If one is interested in an unsupervised classification, it is commom to use *internal validity indices*, such as Silhouettes [@Rousseeuw1987], to help determine the best number of clusters. However, if one is interested in understand the structure of a labeled data set, or in the identification of sample anomaly, as we are here, one can reccur to *external validity indices* to assist the semisupervised procedure that achieves the optimal correspondence between the clusters and classes partitions. In this regard, we need to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to its known classes. To help this process, `sits` provides `sits_dendro_bestcut()` function that computes the external validity index *adjusted Rand index* (ARI) for a series of different number of generated clusters. The function returns the height where the cut of the dendrogram maximizes the index.

```{r}
# search for the best height to cut the dendrogram
sits_dendro_bestcut(cerrado_2classes, 
                    dendro)
```

This height optmises the ARI and generates $6$ clusters. The ARI considers any pair of distinct samples and computes the following counts:
a) the number of distinct pairs whose samples have the same label and are in the same cluster;
b) the number of distinct pairs whose samples have the same label and are in different clusters;
c) the number of distinct pairs whose samples have different labels and are in the same cluster;
d) the number of distinct pairs whose samples have the different labels and are in different clusters.
Here, $a$ and $d$ consists in all agreements, and $b$ and $c$ all disagreements. The ARI is obtained by
$$
ARI=\frac{a+d-e}{a+d+b+c-e},
$$
where $e$ is the expected agreement, a random chance correction [@Hubert1985]. Different from others validity index such as Jaccard and Fowlkes-Mallows indices, ARI is more appropriate either when the nuber of clusters is outwheighted by the number of labels (or conversely) and the number of samples is imbalanced between labels and clusters (which is usually the case).

```{r}
# create 6 clusters by cutting the dendrogram at the linkage distance 20.39655
clusters.tb <- 
    sits_cluster(cerrado_2classes, 
                 dendro, 
                 k = 6)
# show clusters samples frequency
sits_cluster_frequency(clusters.tb)
```

Note in this example that almost all clusters has a predomination of either "Cerrado" or "Pasture" classes with the exception of cluster $3$. The contigency table ploted by `sits_cluster_frequency` is useful to identify anomalies in samples in a qualitative fashion. If one considers such cases as outliers, it is possible to exclude them using the function `sits_cluster_cleaner()` that removes all samples whose label (cluster) counts less than a given percentual relative to its cluster (label).

```{r}
# clear sample outliers relative to clusters 
# (those with less than 5% relative to intracluster)
clusters2.tb <- sits_cluster_cleaner(clusters.tb)
# show clusters samples frequency
sits_cluster_frequency(clusters2.tb)
```


<!-- subsituir por uma discussao sobre o uso de clustering em SITS -->
<!-- We could proceed by cutting the dendrogram at lower linkage distances to obtain other set of clusters. We can note that at some point it is possible to achieve a reasonable separability between classes (in our example, this is indicated by the dashed line drawed according to `cutree_height` parameter). The dendrogram thus provides users with a powerful visual aid to understand data separability. Depending on the level of dendrogram cut, we will find more or less confusion inside the clusters.-->

The dendrogram can be used to evaluate cluster separability in a qualitative fashion, and helps the practitioner to determine where to cut the tree to obtain the final clusters [@Liao2005].


# Filtering techniques 

Satellite image time series will always be contaminated by atmospheric influence, geolocation error, and directional effects [@Lambin2006]. In tropical regions, clouds are a major factor on reducing the quality of the data. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on an year to year basis [@Atkinson2012]. As we can note in The same land use and land cover class is subject to intrinsic year to year variability. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with data sets that are *noisy* and *non-homogeneous*.

Several techniques have been used to correct and smooth such time-series vegetation index data, and to support the estimation of phenological parameters. These methods include curve fitting [@Bradley2007], asymmetric Gaussian functions [@Jonsson2002], wavelet decomposition [@Sakamoto2005], the Savitzky–Golay filter [@Chen2004], and the Whittaker smoother [@Atzberger2011]. 

One of the main uses of time series filtering is to reduce the noise produced by clouds in tropical areas. The following examples we use data produced by the PRODES project [@INPE2017], which detects deforestation in the Brazilian Amazon rain forest by visual interpretation.  Its interpreters are highly skilled and the accuracy of PRODES has been essential for Brazil's actions to control Amazon deforestation [@Rajao2009]. Our time series were generated from a combination of MODIS and Landsat 8 images covering the region corresponding to the standard Landsat Path/Row 226/064. This is an area in the East of the Pará state. The area has been chosen because the area is subject to strong cloud cover from November to March, which is a significant factor in degrading time series quality.

Each location information consists of its latitude and longitude, the start and end dates of an yearly time series, and the label, as showm below. Three land cover classes were identified: "Forest" (natural forest), "Deforestation_2014" and "Deforestation_2015" (areas identified by PRODES in years 2014 and 2015 as clear cuts, respectivelly) and "Pasture" (areas that had been deforested before 2014). 

```{r}
prodes_226_064[1:3,]
```

<!-- All samples span one year, starting in August 1st of an year and ending in July 31st of the next year. This sampling procedure was chosen to reflect the PRODES methodology for estimating deforestation by clear cuts in the Amazon, which uses the same period. In practice, most of the areas detected by PRODES came from images from the months from April to July, when there is less cloud cover in the region.-->

The filtering algorithms provided by `sits` are: Savitzky–Golay filter (`sits_sgolay()`), Whitaker filter (`sits_whittaker()`), envelope filter (`sits_envelope()`) and, the "cloud filter" (`sits_cloud_filter()`). In what follows, we explain each technique and provide some examples using the afore described PRODES data set. To best visualize the effects of filtering we select only NDVI band of the time series.

## Savitzky–Golay filter

The Savitzky-Golay filter works by fitting successive sub-sets of adjacent data points with a low-degree polynomial whose coefficients can be derived by through the method of linear least squares. The resulting $i$-th point of the filtered time series is obtained by substituting the polynomial independent terms by the $n$-adjacent points relative to the central point $i$ in the original time series, as expressed in
$$
{\hat{x}_{i}=\sum _{j=-n}^{n}C_{j}\,x_{i+j}},
$$
where $\hat{x}$ is the the filtered time series, $C_{j}$ is the polynomial coefficient respective to the $j$-th adjacent point and, $x$ is the original time series. The coefficients have analytical solution when the time series is equally spaced and depends uniquely on polynomial degree and the length of the sub set of adjacent data points (including the central point, $2n+1$). More details on the procedure of coefficient determination can be found in @Savitzky1964.

The main purpose of Savitzky-Golay filter is to increase the signal-to-noise ratio with a minimum distortion of the time series. The difference between an original and a filtered time series using the Savitsky-Golay filter is shown in example bellow.

<!-- Colocar bibliografia de uso do SG em SITS -->

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- sits_select(prodes_226_064[1,], bands = c("ndvi"))
# apply Savitzky–Golay filter
point_sg.tb <- sits_sgolay(point.tb)
# plot the series
sits_plot(sits_merge(point_sg.tb, point.tb))
```

## Whittaker filter

<!-- reduzir um pouco a discussao tecnica, falar mais sobre os resultados obtidos em comparacao de metodos -->

The Whittaker smoother attempts to fit a curve that represents the raw data, but is penalized if subsequent points vary too much [@Atzberger2011]. The Whittaker filter is a balancing between the residual to the original data and the "smoothness" of the fitted curve [@deRooi2013]. The residual, as measured by the sum of squares of all $n$ time series points deviations, in matrix form, is given by
$$
RSS=(x - \hat{x})^{T}(x - \hat{x}),
$$
where $x$ and $\hat{x}$ are the original and the filtered time series vectors, respectivelly. The "smoothness" is assumed to be the measure of the the sum of the squares of the third differences [@Whittaker1922], which is given by
$$
\begin{split}
M_3 = (\hat{x}_4 - 3\hat{x}_3 + 3\hat{x}_2 - \hat{x}_1)^2 + (\hat{x}_5 - 3\hat{x}_4 + 3\hat{x}_3 - \hat{x}_2)^2 \\ + \ldots + (\hat{x}_n - 3\hat{x}_{n-1} + 3\hat{x}_{n-2} - \hat{x}_{n-3})^2,
\end{split}
$$
which in matrix form, can be expressed as
$$
\begin{split}
M_3 = \hat{x}^{T}D^{T}D\hat{x},
\end{split}
$$
where
$$
D = \left[\begin{array}{ccccccc}
   1 & -3 & 3 & -1 & 0 & 0 &\cdots \\
   0 & 1 & -3 & 3 & -1 & 0 &\cdots \\
   0 & 0 & 1 & -3 & 3 & -1 & \cdots \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
\right]
$$
is the third order difference matrix.

Whittaker filter is obtained by finding a new time series $\hat{x}$ whose points minimize the function 
$$
X(\hat{x},\lambda{})=RSS+\lambda{}M_3,
$$ 
where $\lambda{}$, a scalar, works as an "smoothing wheight" parameter. The minimization can be obtained by differentiating the equivalent expression $(x - \hat{x})^{T}(x - \hat{x})+\lambda\hat{x}^{T}D^{T}D\hat{x}$ with respect to $\hat{x}$ and equating it to zero, whose solution gives the Whittaker filtered time series
$$
\hat{x} = ({\bf I} + \lambda {D}^{T} D)^{-1}x.
$$

The Whitakker filter can be a large but sparse optimisation problem, as we can note from $D$ matrix. In `sits` implementation, the default $\lambda{}$ parameter is $1.0$. The example bellow depicts an original and a filtered time series.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Whittaker filter applied on a one-year sample NDVI time series."}
# Take the NDVI band of the first sample data set
point.tb <- sits_select(prodes_226_064[1,], bands = c("ndvi"))
# apply Whitaker filter
point_whit.tb <- sits_whittaker(point.tb)
# plot the series
sits_plot(sits_merge(point_whit.tb, point.tb))
```

## Envelope filter

<!-- qual a referencia na literature -->

This filter produces a bounding (superior or inferior) time series relative to an input signal. We can bound a time series signal by two basic operations: upper and lower dilations. Both operations assumes an unitary window to which the dilation occurs. An upper dilation can be defined as
$$
u_i=\max_{k}{(\{x_{k}:\left|k-i\right|\le{}1\})},
$$
whereas an lower dilation is obtained by
$$
l_i=\min_{k}{(\{x_{k}:\left|k-i\right|\le{}1\})}.
$$
Here, $x$ is the input time series and, $k$ and $i$ are vector indices.

The envelope filter function can combine both upper and lower dilations recursively by a sequence of "U" (upper) and "L" (lower) characters passed as parameter. A repeated sequence of a same operation is equivalent to one operation with a larger dilation window. The final result of the recursive operations over the input time series is than returned.

This filter can be useful to remove isolated noises in the input signal. For example, an downward (upward) noise spike formed by one point in the series can be removed by an "UL" ("LU") sequence of dilations. For spikes with two points, we can repeat each operation, for example "UULL" (the default sequence), to increase the dilation window and remove such imperfections on the time series. In the following example we can see an application of `sits_envelope()` function.

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- sits_select(prodes_226_064[1,], bands = c("ndvi"))
# apply envelope filter (remove short downward noises)
point_env.tb <- sits_envelope(point.tb, "UL")
# plot the series
sits_plot(sits_merge(point_env.tb, point.tb))
```

## Cloud filter

This function tries to remove noise from the input time series. It looks the first order difference time serie for points where the value of the difference goes up abruptly. These points are taken as those whose difference is more than a cutoff value which is set by the user. Then, it applies an autoregressive integrated moving average (ARIMA) model to predict the missing values. The parameters of the ARIMA model can be set by the user. Please see arima for the detailed description of parameters p, d, and q.

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- sits_select(prodes_226_064[1,], bands = c("ndvi"))
# apply ARIMA filter
point_cf.tb <- sits_cloud_filter(point.tb, apply_whit = FALSE)
# plot the series
sits_plot(sits_merge(point_cf.tb, point.tb))
```

# Machine learning classification for land use and land cover using satelite image time series

The main advantage using satellite image time series in land use studies is that the time series is methodologically consistent with the very nature of the land covers. Using this kind of data allows focusing on land changes through time. Currently, most studies that use satellite image time series for land classification still use variations of the classical remote sensing image classification methods. Given a series of images, researchers use methods that produce a single composite for the whole series [@Gomez2016]. In their review on this subject, @Gomez2016 discuss 12 papers that use satellite image time series to derive image composites that are later used for classification. @Camara2016 denote these works as taking a \textit{space-first, time-later} approach.

An example of \textit{space-first, time-later} work on big EO data analysis is the work by @Hansen2013. Using more than 650,000 LANDSAT images and processing more than 140 billion pixels, the authors compared data from 2000 to 2010 to produce maps of global forest loss during the decade. A pixel-based classification algorithm was used to process each image to detect forest cover. The method classifies each 2D image one by one. 

In our view, these methods do not use the full potential of satellite image time series. The benefits of remote sensing time series analysis arise when the temporal resolution of the big data set is able to capture the most important changes. Here, the temporal autocorrelation of the data can be stronger than the spatial autocorrelation. Given data with adequate repeatability, a pixel will be more related to its temporal neighbours than to its spatial ones. In this case, *time-first, space-later* methods lead to better results than the *space-first, time-later* approach [@Camara2016].

The `sits` package provides functionality to explore the full depth of satellite image time series data. It treat time series as a feature vector. To be consistent, the procedure aligns all time series from different years by its time proximity considering an given cropping schedule. Once aligned, the feature vector is formed by all pixel "bands". The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars, also known as training data, to infer classes of a larger data set. In the next section we discuss about machine learning techniques supported in `sits` with more detail.

In the training stage, Additional "bands" can be computed to increase the distinction between classes. This method has a deceptive simplicity 

## Support Vector Machine


```{r}
# Retrieve the set of samples for the Mato Grosso region 
# (provided by EMBRAPA) (samples_MT_ndvi) and 
# get a point to be classified (point_ndvi)
class.tb <- sits_classify(point_ndvi,
                          samples_MT_ndvi,
                          ml_method = sits_svm(kernel = "radial", 
                                               cost = 10))
sits_plot(class.tb)
```

## Random Forest

```{r}
# Retrieve the set of samples for the Mato Grosso region 
# (provided by EMBRAPA) (samples_MT_ndvi) and 
# get a point to be classified (point_ndvi)
class.tb <- sits_classify(point_ndvi,
                          samples_MT_ndvi,
                          ml_method = sits_rfor())
sits_plot(class.tb)
```

# Validation techniques

Our experiment consists of the comparison of different methods to obtain the time series prototypes for each class. After obtaining the prototypes, we classified the data using the TWDTW method and used a cross-validation procedure to evaluate the results.

# Final remarks

Current approaches to image time series analysis still use limited number of attributes. A common approach is deriving a small set of phenological parameters from vegetation indices, like beginning, peak, and length of growing season [@Brown2013], [@Kastens2017], [@Estel2015], [@Pelletier2016]. These phenological parameters are then fed in specialised classifiers such as TIMESAT [@Jonsson2004]. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces and with big training data sets [@James2013].

The `sits` uses the full depth of satellite image time series to create larger dimensional spaces. We tested different methods of extracting attributes from time series data, including those reported by @Pelletier2016 and @Kastens2017. Our conclusion is that part of the information in raw time series is lost after filtering or statistical approximation. Thus, the method we developed has a deceptive simplicity: *use all the data available in the time series samples*. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. Our experiments found out that modern statistical models such as support vector machines, and random forests perform better in high-dimensional spaces than in lower dimensional ones. 

# Acknowledgements

We want to thanks all the researchers whom provided data samples used in the examples. Alexandre Coutinho, Julio Esquerdo and Joao Antunes from the Brazilian Agricultural Research Agency (EMBRAPA) provided the samples for "soybean-fallow", "fallow-cotton", "soybean-cotton", "soybean-corn", "soybean-millet", "soybean-sunflower" and "pasture" classes that were collected through farmer interviews and *in-loco* observations from October 2009 until October 2013. Rodrigo Bergotti from INPE whom provided samples for "cerrado" and "forest" classes through high resolution images observations. Damien Arvor (Rennes University) provided ground samples for "soybean-fallow" class.

<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->

