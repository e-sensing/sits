---
output:
    pdf_document:
      citation_package: natbib
      df_print: tibble
      fig_caption: yes
      keep_tex: yes
      template: latex-ms.tex
title: "SITS: Data Analysis and Machine Learning using Satellite Image Time Series"
author:
- name: Rolf Simoes
  affiliation: National Institute for Space Research (INPE), Brazil
- name: Gilberto Camara
  affiliation: National Institute for Space Research (INPE), Brazil
- name: Alexandre Carvalho
  affiliation: Institute for Applied Economics Research (IPEA), Brazil
- name: Victor Maus
  affiliation: International Institute for Applied System Analysis (IIASA), Austria
- name: Gilberto Queiroz
  affiliation: National Institute for Space Research (INPE), Brazil
abstract: "Using time series derived from big Earth Observation data sets is one of the leading research trends in Land Use Science and Remote Sensing. One of the more promising uses of satellite time series is its application for classification of land use and land cover, since our growing demand for natural resources has caused major environmental impacts. Here, we present the open source R package for satellite image time series analysis, the `sits` package. The `sits` provides support on how to use statistical learning techniques with image time series. These methods include linear and quadratic discrimination analysis, support vector machines, random forests and neural networks."
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontfamily: mathdesign
fontfamilyoptions: adobe-utopia
fontsize: 11pt
bibliography: references-sits.bib
csl: plos-one.csl
endnote: false
graphics: true
mathtools: true
vignette: >
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteIndexEntry{SITS: Data Analysis and Machine Learning using Satellite Image Time Series}
---
```{r, include = FALSE}
library(sits)
library(tibble)
library(dtwclust)
```

# Introduction 

Earth observation satellites provide a continuous and consistent set of information about the Earth’s land and oceans. Most space agencies have adopted an open data policy, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a major challenge: *How to design and build technologies that allow the Earth observation community to analyse big data sets?*

The approach taken in the current work is to develop data analysis methods that work with satellite image time series. The time series are obtained by taking calibrated and comparable measures of the same location in Earth at different times. These measures can be obtained by a single sensor (*e.g.*, MODIS) or by combining different sensors (*e.g.*, Landsat 8 and Sentinel-2). If obtained by frequent revisits, the temporal resolution of these data sets can capture the most important land use changes. 

Time series of remote sensing data show that land cover changes do not always occur in a progressive and gradual way, but they may also show periods of rapid and abrupt change followed either by a quick recovery [@Lambin2003]. Analyses of multiyear time series of land surface attributes, their fine-scale spatial pattern, and their seasonal evolution leads to a broader view of land-cover change. Satellite image time series have already been applied to applications such as mapping for detecting forest disturbance [@Kennedy2010], ecology dynamics [@Pasquarella2016], agricultural intensification [@Galford2008] and its impacts on deforestation [@Arvor2012].

In this paper, we present an open source R package for satellite image time series analysis `sits`. The `sits` package provides support on how to use statistical learning techniques with image time series. In a broad sense, statistical learning refers to a class of algorithms for classification and regression analysis [@Hastie2009]. These methods include linear and quadratic discrimination analysis, support vector machines, random forests and neural networks. In a typical classification problem, we have measures that capture class attributes. Based on these measures, referred as training data, one's task is to select a predictive model that allows inferring classes of a larger data set. 

In what follows, we describe the main characteristics of the `sits`. The first part describes the basic data structures used in it and the tools used for visualisation and data exploration. Then we show how to do data acquisition from external sources, with an emphasis on the WTSS (an acronym for Web Time Series Service) [@Ribeiro2015]. The next sections describe filtering and clustering techniques. We then discuss machine learning techniques for satellite image time series data and how to apply them to image time series. Finally, we present validation methods.

# Data Handling and Visualisation Basics in `sits`

The basic data unit in the `sits` package is the "`sits` tibble", which is a way of organizing a set of time series data with associated spatial information. In R, a `tibble` differs from the traditional data frame, insofar as a `tibble` can contain lists embedded as column arguments. Tibbles are part of the `tidyverse`, a collection of R package designed to work together in data manipulation. The `tidyverse` includes packages such as `ggplot2`, `dplyr` and `purrr` [@Wickham2017]. The `sits` makes extensive use of the `tidyverse`. 

For a better explanation of how the "`sits` tibble" works, we will read a data set containing $2,115$ labelled samples of land cover in Mato Grosso state of Brazil. This state has $903,357$ km^2^ of extension, being the third largest state of Brazil. It includes three of Brazil's biomes: Amazonia, Cerrado and Pantanal. It is the most important agricultural frontier of Brazil and is Brazil's largest producer of soybeans, corn and cotton. 

The samples contain time series extracted from the MODIS MOD13Q1 product from 2000 to 2016, provided every $16$ days at $250$-meter spatial resolution in the Sinusoidal projection. Based on ground surveys and high resolution imagery, we selected $2,115$ samples of nine classes: "Forest", "Cerrado", "Pasture", "Soybean-fallow", "Fallow-Cotton", "Sybean-Cotton", "Soybean-Corn", "Soybean-Millet", and "Soybean-Sunflower".

```{r}
# data set of samples
# print the first three samples
samples_MT_9classes[1:3,]
```

The "`sits` tibble" contains data and metadata. The first six columns contain the metadata: spatial and temporal location, label assigned to the sample, and coverage from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the "WGS84" ellipsoid. For example, the first sample has been labelled "Pasture", at location ($-55.1852$, $-10.8387$), and is considered valid for the period (2013-09-14, 2014-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers involved in labelling the samples chose to use the agricultural calendar in Brazil, where the spring crop is planted in the months of September and October, and the autumn crop is planted in the months of February and March. For other applications and other countries, the relevant dates will most likely be different from those used in the example.

The "`sits` tibble" also contains the time series data for each spatiotemporal location. The timeseries data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. 

```{r}
# print the first 10 time series records of the first sample
samples_MT_9classes$time_series[[1]][1:3,]
```

The `sits` package provides functions for data manipulation and displaying information of a "`sits` tibble". For example, the command `sits_labels()` that shows the labels of the sample set and their frequencies.

```{r}
sits_labels(samples_MT_9classes)
```

In many cases, it is useful to relabel the data set. For example, there may be situations when one wants to use a smaller set of labels, since samples in one label on the original set may not be distinguishable from samples with other labels. We then should use `sits_relabel()`, which requires a conversion list (for details, see `?sits_relabel`).

Given that we have used the tibble data format for the metadata and and the embedded time series, one can use the functions of the `dplyr`, `tidyr` and `purrr` packages of the `tidyverse` [@Wickham2017] to process the data. For example, the following code uses the `sits_select()` function to get a subset of the sample data set with two bands (NDVI and EVI) and then uses the `dplyr::filter()` function to select the samples labelled either as "Cerrado" or "Pasture". We can then use the `sits_plot()` function to display the time series. Given a small number of samples to display, the `sits_plot()` function tries to group as many spatial locations together. In the following example, the first 15 samples of the "Cerrado" class all refer to the same spatial location in consecutive time periods. For this reason, these samples are plotted together.



```{r cerrado-15, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of the first 15 'Cerrado' samples from data set \\texttt{samples_MT_9classes} (different dates for the same point location)."}
# select the "ndvi" bands
samples_ndvi.tb <- 
    sits_select(samples_MT_9classes, 
                bands = c("ndvi"))
# select only the samples with the cerrado label
samples_cerrado.tb <- 
    dplyr::filter(samples_ndvi.tb, 
                  label == "Cerrado")
# plot the first 15 samples (different dates for the same points)
sits_plot(samples_cerrado.tb[1:15,])
```

For a large number of samples, where the amount of individual plots would be substantial, the default visualisation combines all samples together in a single temporal interval (even if they are valid for different years). Therefore, all samples of the same band and the same label are aligned to a common interval. This plot is useful to show the spread of values for the time series of each band. The strong red line in the plot shows the median of the values, and the two orange lines are the first and third interquartile ranges. The `sits_plot()` function has different ways of working. Please, refer to the package documentation for more details.

```{r cerrado-all, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of all 'Cerrado' samples from data set \\texttt{samples_MT_9classes}."}
# plot all cerrado samples together (shows the distribution)
sits_plot(samples_cerrado.tb)
```

Usually, samples are provided by experts whose take *in-loco* field observations or recognises land classes through high resolution images. In any case, we need access to a data source to fetch time series data regarding a spatiotemporal location of interest. The process of importing data samples is discussed in the next section.

# Importing Data into `sits`

The `sits` package allows different methods of data input, including: (a) obtain data from a WTSS (Web Series Time Service); (b) obtain data from the SATVEG service developed by EMBRAPA (Brazil's Agriculture Research Agency). (c) read data stored in a time series in the ZOO format [@Zeileis2005]; (d) read a time series from a RasterBrick [@Hijmans2015]. Option (d) will be described in the section were we describe raster processing. The WTSS service is a light-weight service, designed to retrieve time series for selected locations and periods [@Vinhas2016], been implemented by the research team of the National Institute for Space Research to allow remote access to time series data. The SATVEG service provides NDVI and EVI time series vegetation indices from MODIS image from whole Brazilian territory [@Embrapa2014]. To view service details, the user needs to call `sits_services()` that provides information on the coverages available on the server.

After finding out which coverages are available at the different time series services, one may request specific information on each coverage by using  `sits_coverage()`. This lists the contents of the data set, including source, bands, spatial extent and resolution, time range, and temporal resolution. This information is then stored in a tibble for later use.

```{r}
# get information about a specific coverage from WTSS
coverage_wtss <- 
    sits_coverage(service  = "WTSS-INPE-1", 
                  product  = "MOD13Q1", 
                  name     = "mod13q1_512")
coverage_wtss[, c("xmin","xmax","ymin","ymax",
                "start_date", "end_date")]
```

The user can request one or more time series points using `sits_getdata()`. This function provides a general means of access to image time series. In its simplest fashion, the user provides the latitude and longitude of the desired location, the product and coverage names, the bands, and the start date and end date of the time series. If the start and end dates are not provided, all available period is retrived. The result is a tibble that can be visualised using `sits_plot()`.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="NDVI and EVI time series fetched from WTSS service."}
# a point in the transition forest pasture in Northern MT
# obtain a time series from the WTSS server for this point
series.tb <- 
    sits_getdata(longitude  = -55.57320, 
                 latitude   = -11.50566, 
                 coverage   = coverage_wtss, 
                 bands      = c("ndvi", "evi"))
# plot the series
sits_plot(series.tb)
```

A useful case is when users have a set of labelled samples, that are to be used as a training data set. In this case, one usually has trusted observations which are labelled and commonly stored in plain text CSV files. The `sits_getdata()` function can receive a CSV file path as an argument. The CSV file must provide for each time series, its latitude and longitude, the start and end dates, and a label associated to a ground sample. 

After importing the samples time series, it is useful to explore the data and see how is it underlying structured and its inter-class separability. For example, We can note in the figure above the variability of 400 time series samples along time. Those samples were collected from different years and/or locations. The scattering behaviour is intrinsic to remote sensing data. Atmospheric noise, sun angle, interferences on observations or different equipaments specifications, as well as the very nature of the climate-land dynamics can be sources of such variability [@Atkinson2012]. One helpful technique to explore such properties is *cluster analysis*. In the following section we present a cluster technique supported by `sits`. 

# Clustering in satellite image time series

Cluster analysis has been used for many purposes in satellite image time series literature ranging from unsupervised classification [@Petitjean2011], and pattern detection [@Romani2011]. Here, we are interested in the second use of clustering, as a way to improve training data to use in machine learning classification models. In this regard, cluster analysis can assist the identification of structural *time series patterns*, and anomalous samples [@Romani2011], [@Chandola2009]. `sits` provides support for the agglomerative hierarchical clustering (AHC). 

Hierarchical clustering is a family of methods that groups elements using a distance function to associate a real value to a pair of elements. From this distance measure, we can compute the dissimilarity between any two elements from the data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. The AHC approach is suitable for the purposes of samples data exploration awe its visualisation power and ease of use [@Keogh2003]. Moreover, AHC does not require a predefined number of clusters as an initial parameter. This is an important feature in satellite image time series clustering since it is not easy to define the number of clusters present in a set of multi-attribute time series [@Aghabozorgi2015].

The main result of AHC method is the *dendrogram*. A *dendrogram* is the ultrametric relation formed by the successive merges in the hierarchical process that can be represented by a tree. Dendrograms are quite useful to decide on the number of clusters has the data. It shows the height where each merging happened, which corresponds to the minimum distance between two clusters defined by a *linkage criterion*. The most commom linkage criteria are: *single-linkage*, *complete-linkage*, *average-linkage*, and *Ward-linkage*. Complete-linkage prioritises the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, that can increase the resulting intracluster data variance. As an alternative, Ward proposes a criteria to minimise the data variance by means of either *sum-of-squares* or *sum-of-squares-error* [@Ward1963]. Ward's intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape [@Hennig2015]. In `sits`, a dendrogram can be generated by `sits_dendrogram()`. The following codes illustrate how to create, visualise, and cut a dendrogram (for details, see `?sits_dendrogram()`).

```{r dendrogram, cache=TRUE, fig.align="center", fig.height=4.1, fig.width=5}
# take a set of patterns for 2 classes
# create a dendrogram object with default clustering parameters
dendro <- sits_dendrogram(cerrado_2classes)
# plot the resulting dendrogram
sits_plot_dendrogram(cerrado_2classes, 
                     dendro)
```

After the creation of a dendrogram, an important question emerges: *where to cut the dendrogram?* The answer depends on what are the purposes of the cluster analysis [@Hennig2015]. If one is interested in an unsupervised classification, it is commom to use *internal validity indices*, such as Silhouettes [@Rousseeuw1987], to help determine the best number of clusters. However, if one is interested in understand the structure of a labeled data set, or in the identification of sample anomaly, as we are here, one can reccur to *external validity indices* to assist the semisupervised procedure that achieves the optimal correspondence between the clusters and classes partitions. In this regard, we need to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to its known classes. To help this process, `sits` provides `sits_dendro_bestcut()` function that computes the external validity index *adjusted Rand index* (ARI) for a series of different number of generated clusters. The function returns the height where the cut of the dendrogram maximizes the index.

```{r}
# search for the best height to cut the dendrogram
sits_dendro_bestcut(cerrado_2classes, 
                    dendro)
```

This height optmises the ARI and generates $6$ clusters. The ARI considers any pair of distinct samples and computes the following counts:
a) the number of distinct pairs whose samples have the same label and are in the same cluster;
b) the number of distinct pairs whose samples have the same label and are in different clusters;
c) the number of distinct pairs whose samples have different labels and are in the same cluster;
d) the number of distinct pairs whose samples have the different labels and are in different clusters.
Here, $a$ and $d$ consists in all agreements, and $b$ and $c$ all disagreements. The ARI is obtained by
$$
ARI=\frac{a+d-E}{a+d+b+c-E},
$$
where $E$ is the expected agreement, a random chance correction calculated by 
$$
E=(a+b)(b+c)+(c+d)(b+d).
$$ 

Different from others validity index such as Jaccard ($J=a/(a+b+c)$), Fowlkes-Mallows ($FM=a/(a^2+a(b+c)+bc)^{1/2}$), and Rand (the same as ARI without the $E$ adjustment) indices, ARI is more appropriate either when the number of clusters is outwheighted by the number of labels (and *vice versa*) and the amount of samples in labels and clusters is imbalanced [@Hubert1985], which is usually the case.

```{r}
# create 6 clusters by cutting the dendrogram at 
# the linkage distance 20.39655
clusters.tb <- 
    sits_cluster(cerrado_2classes, 
                 dendro, 
                 k = 6)
# show clusters samples frequency
sits_cluster_frequency(clusters.tb)
```

Note in this example that almost all clusters has a predomination of either "Cerrado" or "Pasture" classes with the exception of cluster $3$. The contigency table ploted by `sits_cluster_frequency` shows how the samples are distributed across the clusters and helps to identify two kinds of confusions. The first is relative to those small amount of samples in clusters dominated by another class (*e.g.* clusters $1$, $2$, $4$, $5$, and $6$), and the second is relative to those samples in non-dominated clusters (*e.g.* cluster $3$). These confusions can be an indication of poor quality of samples, or an inadequacy between the used parameters in cluster analysis, or even a natural confusion due to the inherent variability of the land classes.

For whatever reason, one can check other methods to assist one's decision, either by eliminating part of it, or by improving the data set. If one considers such cases as outliers, it is possible to remove them using the functions `sits_cluster_clean()` and `sits_cluster_remove()`. The first removes all those minority samples that do not reach a minimum percentage close to $0\%$, whereas the second removes an entire cluster if its dominant class does not reach a minimum percentage, close to $100\%$. The example illustrates the second aproach.

```{r}
# clear those samples with a high confusion rate in a cluster 
# (those clusters which majority class does not reach 90% of 
#  samples in that cluster)
cleaned.tb <- 
    sits_cluster_remove(clusters.tb, 
                        min_perc = 0.9)
# show clusters samples frequency
sits_cluster_frequency(cleaned.tb)
```

Along the process of cluster analysis, it may be a good practice to measure the correspondence between clusters and labels partitions through computation of external validity indices. These measures can help the comparison among different procedures and assits the decision-making. `sits_cluster_validity()` provides a way to compute some external validation indices other than ARI (for details, see `?sits_cluster_validity()`). Moreover, these indices capture some of the cluster structure that is present in the correspondence of its partitions [@Hubert1985].

# Filtering techniques 

Satellite image time series generally is contaminated by atmospheric influence, geolocation error, and directional effects [@Lambin2006]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on an year to year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with data sets that are *noisy* and *non-homogeneous*. In this section we discuss filtering techniques, a method used to improve time series data that presents missing values or noise.

The literature on satellite image time series have several applications of filtering used to correct or smooth vegetation index data. The `sits` have support for Savitzky–Golay (`sits_sgolay()`), Whitaker (`sits_whittaker()`), envelope (`sits_envelope()`) and, the "cloud filter" (`sits_cloud_filter()`) filters. The first two filters are commonly used in the literature. The remaining two are adaptations of other models and, for our knowledge, its use have not been reported in the literature.

Various somewhat conflicting results have been expressed in relation to the time series filtering techniques for phenology applications. For example, in an investigation of phenological parameter estimation, @Atkinson2012 found that the Whittaker and Fourier transform approaches were preferable to the double logistic and asymmetric Gaussian models. They applied the filters to preprocess MERIS NDVI time series for estimating phenological parameters in India. Comparing the same filters as in the previous work, @Shao2016 found that only Fourier transform and Whittaker techniques improved interclass separability for crop classes and significantly improved overall classification accuracy. The authors used MODIS NDVI time series from the Great Lakes region in North America. @Zhou2016 found that asymmetric Gaussian model outperforms other filters over high latitude boreal biomes, while the Savitzky-Golay model gives the best reconstruction performance in tropical evergreen broadleaf forests. In the remaining biommes, Whittaker gives superior results. The authors compare all previous mentioned filters plus Savitzky-Golay method for noise removal in MODIS NDVI data from sites spreaded worldwide in different climatological conditions. Many other techniques can be found in applications on satelite image time series such as curve fitting [@Bradley2007], wavelet decomposition [@Sakamoto2005], and mean-value iteration, ARMD3-ARMA5, and 4253H [@Hird2009] techniques. This shows that comparative analysis of smoothing algorithms depends on the performance measure adopted by the authors.

One of the main uses of time series filtering is to reduce the noise and miss data produced by clouds in tropical areas. The following examples use data produced by the PRODES project [@INPE2017], which detects deforestation in the Brazilian Amazon rain forest by visual interpretation. `sits` provides $617$ samples from a region corresponding to the standard Landsat Path/Row 226/064. This is an area in the East of the Brazilian Pará state and has been chosen because its strongly cloud cover from November to March, which is a significant factor in degrading time series quality. Its NDVI and EVI time series were extracted from a combination of MOD13Q1 and Landsat8 images (to best visualize the effects of each filter, we selected only NDVI time series).

## Savitzky–Golay filter

The Savitzky-Golay filter works by fitting a successive array of $2n+1$ adjacent data points with a $d$-degree polynomial through linear least squares. The central point $i$ of the window array assumes the value of the interpolated polynomial. An equivalent and much faster solution than this convolution procedure is given by the closed expression
$$
{\hat{x}_{i}=\sum _{j=-n}^{n}C_{j}\,x_{i+j}},
$$
where $\hat{x}$ is the the filtered time series, $C_{j}$ are the Savitzky-Golay smoothing coefficients, and $x$ is the original time series.

The coeficients $C_{j}$ depend uniquely on the polynomial degree ($d$) and the length of the window data points (given by parameter $n$). If $d=0$, the coeficients are constants $C_{j}=1/(2n+1)$ and the Savitzky-Golay filter will be equivalent to moving average filter. When the time series is equally spaced, the coefficients have analytical solution. According to @Madden1978, for $d\in{}[2,3]$ each $C_{j}$ smothing coefficients can be obtained by
$$
C_{j}=\frac{3(3n^2+3n-1-5j^2)}{(2n+3)(2n+1)(2n-1)}.
$$

In general, the Savitzky-Golay filter produces smoother results for a larger value of $n$ and/or a smaller value of $d$ [@Chen2004]. The optimal value for these two parameters can vary from case to case. For example, @Zhou2016 set $d=2$ and $n=10$. @Hird2009 tests the filter for $n\in{}[5,6,7]$ using quadratic polynomial.

`sits` Savitzky-Golay function  The following example shows the effect of Savitsky-Golay filter on the original time series.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Savitzky-Golay filter applied on a one-year NDVI time series."}
# Take the NDVI band of the first sample data set
point.tb <- 
    sits_select(prodes_226_064[1,], 
                bands = c("ndvi"))
# apply Savitzky–Golay filter
point_sg.tb <- sits_sgolay(point.tb)
# plot the series
sits_plot(sits_merge(point_sg.tb, point.tb))
```

## Whittaker filter

The Whittaker smoother attempts to fit a curve that represents the raw data, but is penalized if subsequent points vary too much [@Atzberger2011]. The Whittaker filter is a balancing between the residual to the original data and the "smoothness" of the fitted curve. The residual, as measured by the sum of squares of all $n$ time series points deviations, is given by
$$
RSS=\sum_{i}(x_{i} - \hat{x_{i}})^2,
$$
where $x$ and $\hat{x}$ are the original and the filtered time series vectors, respectivelly. The smoothness is assumed to be the measure of the the sum of the squares of the third order differences of the time series [@Whittaker1922] which is given by
$$
\begin{split}
SSD_3 = (\hat{x}_4 - 3\hat{x}_3 + 3\hat{x}_2 - \hat{x}_1)^2 + (\hat{x}_5 - 3\hat{x}_4 + 3\hat{x}_3 - \hat{x}_2)^2 \\ + \ldots + (\hat{x}_n - 3\hat{x}_{n-1} + 3\hat{x}_{n-2} - \hat{x}_{n-3})^2.
\end{split}
$$

Whittaker filter is obtained by finding a new time series $\hat{x}$ whose points minimize the expression
$$
RSS+\lambda{}SSD_3,
$$ 
where $\lambda{}$, a scalar, works as an "smoothing wheight" parameter. The minimization can be obtained by differentiating the expression with respect to $\hat{x}$ and equating it to zero. The solution of the resulting linear system of equations gives the filtered time series which, in matrix form, can be expressed as
$$
\hat{x} = ({\rm I} + \lambda {D}^{T} D)^{-1}x,
$$
where ${\rm I}$ is the identity matrix and 
$$
D = \left[\begin{array}{ccccccc}
1 & -3 & 3 & -1 & 0 & 0 &\cdots \\
0 & 1 & -3 & 3 & -1 & 0 &\cdots \\
0 & 0 & 1 & -3 & 3 & -1 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
\right]
$$
is the third order difference matrix. The Whitakker filter can be a large but sparse optimisation problem, as we can note from $D$ matrix. 

Whittaker smoother has been used only recently in satellite image time series investigations. According to @Atzberger2011, the smoother has an advantage over other filtering techniques such as Fourier and wavelets as it does not assume signal periodicity. Moreover, the authors argues that Whittaker permits a rapid processing of large amounts of data, and handles easily incomplete time series with missing values. The fact that it has only one parameter ($\lambda{}$) facilitates its calibration/comparison process. @Zhou2016 found that $\lambda=15$ gives the best result when compared with $\lambda=2$. Larger values of $\lambda{}$ produces smoother results.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Whittaker smoother filter applied on one-year NDVI time series. The example uses default $\\lambda=1$ parameter."}
# Take the NDVI band of the first sample data set
point.tb <- 
    sits_select(prodes_226_064[1,], 
                bands = c("evi"))
# apply Whitaker filter
point_whit.tb <- sits_whittaker(point.tb)
# plot the series
sits_plot(sits_merge(point_whit.tb, 
                     point.tb))
```

## Envelope filter

This filter can generate a time series corresponding to the superior (inferior) bounding of the input signal. This is acomplished through a convoluting window (odd length) that attributes to the point $i$, in the resulting time series, the maximimum (minimum) value of the points in the window. The $i$ point corresponds to the central point of the window. It can be defined as
$$
u_i=\max_{k}{(\{x_{k}:\left|i-k\right|\le{}1\})},
$$
whereas an lower dilation is obtained by
$$
l_i=\min_{k}{(\{x_{k}:\left|i-k\right|\le{}1\})}.
$$
Here, $x$ is the input time series and, $k$ and $i$ are vector indices.

The `sits_envelope()` can combine both maximum and minimum window sequencially. The function can receive a string sequence with `"U"` (for maximization) and `"L"` (for minimization) characters passed to its parameter. A repeated sequence of the same character is equivalent to one operation with a larger window. The sequencial operations on the input time series produces the final filtered result that is returned.

The envelope filter can be viewed through mathematical morphology lenses, a very common field in digital image processing [@Haralick1987]. Here the operations of `"U"` and `"L"` corresponds to the *dilation* and *erosion* morphological operators applied to univariate arrays [@Vavra2004]. Furthermore, the compounds operation of *opening* and *closing* can be obtained by `"UL"` and `"LU"`, respectivelly. This technique has been applied on time series analysis in other fields [@Accardo1997] but, for our knowledge, there is no application in satellite image time series literature.

In the following example we can see an application of `sits_envelope()` function. There, we performs the *opening filtration* and *closing filtration* introduced by @Vavra2004. The correspondent operations sequence are `"ULLULUUL"` and `"LUULULLU"`.

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- 
    sits_select(prodes_226_064[1,], 
                bands = c("ndvi"))
# apply envelope filter (remove downward and upward noises)
point_env1.tb <- 
    sits_envelope(point.tb,
                  "ULLULUUL",
                  bands_suffix = "OF")
point_env2.tb <- 
    sits_envelope(point.tb,
                  "LUULULLU",
                  bands_suffix = "CF")
# plot the series
sits_plot(
    sits_merge(
        sits_merge(point_env1.tb, 
                   point_env2.tb),
        point.tb))
```

## Cloud filter

The cloud filter makes use of the well known autoregressive integrated moving average (ARIMA) model. The algorithm looks to the first order difference time series for points where the value is above a certain threshold. This procedure selects only those points with large variations in the original time series, probably associated with noise. Finally, these points are replaced by the ARIMA correspondent values.

The parameters of the ARIMA model can be set by the user. Please see arima for the detailed description of parameters $p$, $d$, and $q$.

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- sits_select(prodes_226_064[1,], bands = c("ndvi"))
# apply ARIMA filter
point_cf.tb <- sits_cloud_filter(point.tb, apply_whit = FALSE)
# plot the series
sits_plot(sits_merge(point_cf.tb, point.tb))
```

# Machine learning classification for land use and land cover using satelite image time series

The main advantage using satellite image time series in land use studies is that the time series is methodologically consistent with the very nature of the land covers. Using this kind of data allows focusing on land changes through time. Currently, most studies that use satellite image time series for land classification still use variations of the classical remote sensing image classification methods. Given a series of images, researchers use methods that produce a single composite for the whole series [@Gomez2016]. In their review on this subject, @Gomez2016 discuss $12$ papers that use satellite image time series to derive image composites that are later used for classification. @Camara2016 denote these works as taking a *space-first*, *time-later* approach.

An example of *space-first*, *time-later* work on big EO data analysis is the work by @Hansen2013. Using more than $650,000$ Landsat images and processing more than $140$ billion pixels, the authors compared data from 2000 to 2010 to produce maps of global forest loss during the decade. A pixel-based classification algorithm was used to process each image to detect forest cover. The method classifies each 2D image one by one.

In our view, these methods do not use the full potential of satellite image time series. The benefits of remote sensing time series analysis arise when the temporal resolution of the big data set is able to capture the most important changes. Here, the temporal autocorrelation of the data can be stronger than the spatial autocorrelation. Given data with adequate repeatability, a pixel will be more related to its temporal neighbours than to its spatial ones. In this case, *time-first, space-later* methods lead to better results than the *space-first, time-later* approach [@Camara2016].

The `sits` package provides functionality to explore the full depth of satellite image time series data. It treat time series as a feature vector. To be consistent, the procedure aligns all time series from different years by its time proximity considering an given cropping schedule. Once aligned, the feature vector is formed by all pixel "bands". The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars (the training data) to infer classes of a larger data set. In the following sections we discuss on machine learning techniques supported in `sits` with more detail.

## Support Vector Machine

Support Vector Machine (SVM) is a generalization of the *separating hyperplane classifier* [@Hastie2009]. In its simpler formulation, SVM finds linear boundaries in the input feature space. A boundary is a hyperplane that divides the entire feature space into two parts. Let $(x, y)$ be a sample observation with attribute vector $x\in{\mathbb{R}^p}$ and class $y\in\{-1,1\}$. Once feature space has $p$ dimensions, the hyperplane has dimension $p-1$ and can be written as
$$
a_0+a_1x_{1}+a_2x_{2}+\ldots+a_px_{p}=0,
$$
where $x_{i}$ is the $i$-th attribute of $x$.

If the samples are separable, the hyperplane has a perfect classification property, that is, for a given sample $(x_k, y_k)$, the following relation holds
$$
\begin{split}
a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp}<0,&\quad{}{y_k=-1},\\
a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp}>0,&\quad{}{y_k=1}.
\end{split}
$$

The hyperplane coeficients define a decision boundary that divides the attribute space separating the samples of distinct classes. Moreover, this division must be achieved by maximizing the margin $M$ between the hyperplane and the attributes vectors for class $1$ and $-1$. One can show that the orthogonal distance between any point $k$ and the hyperplane is
$$
(a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp})y_k\ge{}0,
$$
if we constraint the coeficients to
$$
\sum_{i=1}^{p}{a_{i}^2}=1.
$$
Hence, for all sample $(x_k, y_k)$, the optimization problem can be stated as [@James2013]
$$
\begin{split}
&\max_a{M}\\
&{\rm subject\ to}\\
&\sum_{i=1}^{p}{a_{i}^2}=1,\\
&(a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp})y_k\ge{}M.
\end{split}
$$

The condition for which this optimization problem has solution is too restrictive as it needs the perfect separability of the two classes by an hyperplane. However, in a typical classification problem, observations of two classes are not necessarily separable by a hyperplane. When this is the case, there is at least two samples for which the expression  $(a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp})y_k\ge{}0$ does not hold. Under such condition, $M$ would goes bellow zero without any restriction leading the classifier to be extremely sensitive to a change in a single observation. To overcome this undesired behavior, SVM introduces a softner term in the optimization problem. This modification allow some observations to be on the incorrect part of the feature space violating tha maximum margin classifier but increaing the robustness against individual observations influence. The soft margin optimization problem can be stated as [@James2013]
$$
\begin{split}
&\max_{a,\epsilon}{M}\\
&{\rm subject\ to}\\
&\sum_{i=1}^{p}{a_{i}^2}=1,\\
&(a_0+a_1x_{k1}+a_2x_{k2}+\ldots+a_px_{kp})y_k\ge{}M(1-\epsilon_k),\\
&\epsilon_k\ge{}0,\quad\sum_{k=1}^{n}{\epsilon_{k}}\le{}C,
\end{split}
$$
where $x_{ki}$ is the $i$-th attribute of $x_k$ sample attribute vector, $\epsilon_k$ is a slack variable of the $k$-th sample, and $C$ is a tunning parameter that is tipically chosen by a cross-validation procedure. 

<!-- How is the solution? -->

The solution $\hat{a}$ and $\hat{\epsilon}$ of this optimization problem takes into consideration only those sample points that have strictly positive $\epsilon$ values (all those points that violates the *maximum margin* criteria), the so-called *support vectors*. All other points does not have any influence on $\hat{a}$ values.

Now, we can define the hyperplane function $f(x)=\hat{a}_0+\hat{a}_1x_{1}+\hat{a}_2x_{2}+\ldots+\hat{a}_px_{p}$ that classifies any test feature vector $x^*$ according to
$$
\begin{split}
y^*=1&\quad f(x^*)>0,\\
y^*=-1&\quad f(x^*)<0.
\end{split}
$$

This approach can 
...
most or all samples of a given class belongs in the same side. However, when the relationship between a class and its predictors is non-linear, hyperplanes can suffer ... 

```{r, fig.align="center", fig.height=3.4, fig.width=5.5}
# Retrieve the set of samples for the Mato Grosso region 
# (provided by EMBRAPA) (samples_MT_ndvi) and 
# get a point to be classified (point_ndvi)
class.tb <- 
    sits_classify(point_ndvi,
                  samples_MT_ndvi,
                  ml_method = sits_svm(kernel = "radial",
                                       cost = 10))
sits_plot(class.tb)
```

.
## Random Forest

```{r, fig.align="center", fig.height=3.4, fig.width=5.5}
# Retrieve the set of samples for the Mato Grosso region 
# (provided by EMBRAPA) (samples_MT_ndvi) and 
# get a point to be classified (point_ndvi)
class.tb <- 
    sits_classify(point_ndvi,
                  samples_MT_ndvi,
                  ml_method = sits_rfor())
sits_plot(class.tb)
```

# Validation techniques

Validation is a process undertaken on models to estimate some error associated with them, and hence has been used widely in different scientific disciplines. Here, we are interested in estimating the prediction error associated to some model. For this purpose, we concentrate on the *cross-validation* approach, probably the most used validation technique [@Hastie2009].

To be sure, cross-validation estimates the expected prediction error. It uses part of the available samples to fit the classification model, and a different part to test it. The so-called *k-fold* validation, we split the data into $k$ partitions with approximately the same size and proceed by fitting the model and testing it $k$ times. At each step, we take one distinct partition for test and the remaining $k-1$ for training the model, and calculate its prediction error for classifing the test partition. A simple average gives us an estimation of the expected prediction error. 

A natural question that arises is: *how good is this estimation?* According to @Hastie2009, there is a bias-variance trade-off in choice of $k$. If $k$ is set to the number of samples, we obtain the so-called *leave-one-out* validation, the estimator gives a low bias for the true expected error, but produces a high variance expectation. This can be computational expensive as it requires the same number of fitting process as the number of samples. On the other hand, if we choose $k=2$, we get a high biased expected prediction error estimation that overestimates the true prediction error, but has a low variance. The recommended choices of $k$ are $5$ or $10$ [@Hastie2009], which somewhat overestimates the true prediction error. In satellite time series literature we find ...

`sits_kfold_validate` gives support the k-fold validation 
...

```{r}
# read a set of samples
data (cerrado_2classes)

# perform a five fold validation with the 
# SVM machine learning method using default parameters
prediction.mx <- 
    sits_kfold_validate(cerrado_2classes, 
                        folds = 5)
# prints the output confusion matrix and statistics 
sits_conf_matrix(prediction.mx)
```

# Raster classification

```{r, fig.align="center", fig.height=3.4, fig.width=4.1}
# Retrieve the set of samples for the Mato Grosso region 
data(samples_MT_ndvi)

# read a raster file and put it into a vector
files  <- 
    system.file("extdata/raster/mod13q1/sinop-crop-ndvi.tif", 
                package = "sits")

# define the timeline
data(timeline_mod13q1)
timeline <- lubridate::as_date(timeline_mod13q1$V1)

# create a raster metadata file based on the information about the files
raster.tb <- 
    sits_coverage(service = "RASTER",
                  product = "MOD13Q1",
                  name = "Sinop-crop",
                  timeline = timeline,
                  bands = c("ndvi"),
                  files = files)

# classify the raster file
raster_class.tb <- 
    sits_classify_raster(file = "./raster-class", 
                         raster.tb, 
                         samples_MT_ndvi,
                         ml_method = sits_svm(), 
                         blocksize = 300000, 
                         multicores = 1)
# plot classified image
plot(raster_class.tb$r_obj[[1]])
```

```{r, include=FALSE}
# remove all files
file.remove(unlist(raster_class.tb$file))
```

# Final remarks

Current approaches to image time series analysis still use limited number of attributes. A common approach is deriving a small set of phenological parameters from vegetation indices, like beginning, peak, and length of growing season [@Brown2013], [@Kastens2017], [@Estel2015], [@Pelletier2016]. These phenological parameters are then fed in specialised classifiers such as TIMESAT [@Jonsson2004]. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces and with big training data sets [@James2013].

The `sits` uses the full depth of satellite image time series to create larger dimensional spaces. We tested different methods of extracting attributes from time series data, including those reported by @Pelletier2016 and @Kastens2017. Our conclusion is that part of the information in raw time series is lost after filtering or statistical approximation. Thus, the method we developed has a deceptive simplicity: *use all the data available in the time series samples*. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. Our experiments found out that modern statistical models such as support vector machines, and random forests perform better in high-dimensional spaces than in lower dimensional ones. 

# Acknowledgements

The authors would like to thank all the researchers whose provided data samples used in the examples: Alexandre Coutinho, Julio Esquerdo and Joao Antunes (Brazilian Agricultural Research Agency, Brazil) who provided ground samples for "soybean-fallow", "fallow-cotton", "soybean-cotton", "soybean-corn", "soybean-millet", "soybean-sunflower" and "pasture" classes; Rodrigo Bergotti (National Institute for Space Research, Brazil) who provided samples for "cerrado" and "forest" classes; and Damien Arvor (Rennes University, France) who provided ground samples for "soybean-fallow" class.

<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->

