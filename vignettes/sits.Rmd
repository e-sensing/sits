---
output:
    pdf_document:
      citation_package: natbib
      df_print: tibble
      fig_caption: yes
      keep_tex: no
      template: "../inst/extdata/markdown/latex-ms.tex"
title: 'SITS: Data Analysis and Machine Learning using Satellite Image Time Series'
author:
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Rolf Simoes
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Gilberto Camara
- affiliation: Institute for Applied Economics Research (IPEA), Brazil
  name: Alexandre Carvalho
- affiliation: International Institute for Applied System Analysis (IIASA), Austria
  name: Lorena Santos
- affiliation: International Institute for Applied System Analysis (IIASA), Austria
  name: Victor Maus
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Pedro R. Andrade
- affiliation: National Institute for Space Research (INPE), Brazil
  name: Gilberto Queiroz
date: "`r format(Sys.time(), '%B %d, %Y')`"
endnote: false
fontfamily: mathdesign
fontfamilyoptions: adobe-utopia
fontsize: 11pt
graphics: true
mathtools: true
bibliography: ../inst/extdata/markdown/references-sits.bib
abstract: Using time series derived from big Earth Observation data sets is one of
  the leading research trends in Land Use Science and Remote Sensing. One of the more
  promising uses of satellite time series is its application for classification of
  land use and land cover, since our growing demand for natural resources has caused
  major environmental impacts. Here, we present an open source *R* package for satellite
  image time series analysis called `sits`. Package `sits` provides support on how
  to use statistical learning techniques with image time series. These methods include
  linear and quadratic discrimination analysis, support vector machines, random forests,
  and neural networks.
vignette: |
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteIndexEntry{SITS: Data Analysis and Machine Learning using Satellite Image Time Series}
---
```{r, include = FALSE}
library(sits)
library(tibble)
library(dtwclust)
```

# Introduction 

Earth observation satellites provide a regular and consistent set of information about the land and oceans of the planet. Recently, most space agencies have adopted open data policies, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a major challenge: *How to design and build technologies that allow the Earth observation community to analyse big data sets?*

The approach taken in the current work is to develop data analysis methods that work with satellite image time series,obtained by taking calibrated and comparable measures of the same location in Earth at different times. These measures can be obtained by a single sensor (e.g., MODIS) or by combining different sensors (e.g., Landsat 8 and Sentinel-2). If obtained by frequent revisits, the temporal resolution of these data sets can capture important land use changes. 

Time series of remote sensing data show that land cover can occur not only in a progressive and gradual way, but they may also show discontinuities with abrupt changes [@Lambin2003]. Analyses of multiyear time series of land surface attributes, their fine-scale spatial pattern, and their seasonal evolution leads to a broader view of land-cover change. Satellite image time series have already been used in applications such as mapping for detecting forest disturbance [@Kennedy2010], ecology dynamics [@Pasquarella2016], agricultural intensification [@Galford2008], and its impacts on deforestation [@Arvor2012].

In this work, we present `sits`, an open source R package for satellite image time series analysis. It provides support on how to use machine learning techniques with image time series. These methods include linear and quadratic discrimination analysis, support vector machines, random forests, and neural networks. One important contribution of the `sits` package is to support the complete cycle of data analysis for time series classification, including data acquisition, visualisation, filtering, clustering, classification, validation and post-classification adjustments. 

In what follows, we describe the main characteristics of the `sits` package. The first part describes its basic data structures and the tools used for visualization and data exploration. Then we describe data acquisition from external sources. The next sections describe filtering and clustering techniques. We then discuss machine learning techniques for satellite image time series data and how to apply them to image time series. Finally, we present validation and post-classification methods.

# Previous Work and Research Contribution

Most studies using satellite image time series for land cover classification use a \emph{space-first, time-later} approach. For multiyear studies, researchers first derive best-fit yearly composites and then classify each composite image. For a review of these methods for land use and land cover classification using time series, see [@Gomez2016]. As an alternative to \emph{Space-first, time-later} methods, the `sits` package provides support for classification of time series, preserving the full temporal resolution of the input data, using a \emph{time-first, space-later} approach.

Algorithms for processing image time series using a \emph{time-first, space-later} approach include BFAST for detecting breaks [@Verbesselt2010], TIMESAT for modelling and measuring phenological attributes [@Jonsson2004] and methods based on Dynamic Time Warping (DTW) for land use and land cover classification [@Maus2016}. In all these cases, the authors have provided software that allows researchers to use these methods. 

There has been much recent interest in using classifiers such as support vector machines \citep{Mountrakis2011} and random forests \citep{Belgiu2016} for remote sensing images. Most often, researchers use a \emph{space-first, time-later} approach, in which the dimension of the decision space is limited to the number of spectral bands or their transformations. Sometimes, the decision space is extended with temporal attributes.  To do this, researchers filter the raw data to get smoother time series \citep{Brown2013, Kastens2017}. Then, using software such as TIMESAT \citep{Jonsson2004}, they derive a small set of phenological parameters from vegetation indexes, like the beginning, peak, and length of the growing season \citep{Estel2015, Pelletier2016}. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces with big training data sets \citep{James2013}. They have one thing in common:  raw time series data is considered too noisy to be used directly. This questions the impact of the noise removal and homogenization steps since it may reduce the information present in the SITS.

An alternative approach, proposed in this paper, is to use the full depth of SITS to create larger dimensional spaces for machine learning. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. Each temporal instance of a time series is taken as an independent dimension in the feature space of the classifier. By choosing a statistical classifier which is robust with respect to noise, one should be able to achieve better results than using the current approaches. Thus, the method we developed has a deceptive simplicity: \emph{use all the data available in the time series samples}. To the authors' best knowledge, the classification techniques included in the `sits` package are not previoulsy available in other R or python packages. Furthermore, the package includes methods for filtering, clustering and post-processing that also have not been published in the literature. 

# Data Handling and Visualisation Basics in `sits`

The basic data unit of `sits` package is the "`sits` tibble", a way of organizing a set of time series data with associated spatial information. In *R*, a `tibble` is a generalization of a `data.frame`. A  `data.frame` is the usual way in R to organise data in a table in which each column contains values of one variable and each row contains one set of values from each column. A `tibble` can contain lists embedded as column arguments. Tibbles are part of the `tidyverse`, a collection of R packages designed to work together in data manipulation [@Wickham2017]. The `sits` makes extensive use of the `tidyverse`. As a example of how the "`sits` tibble" works, the following code shows the first three lines of a tibble containing $2,115$ labelled samples of land cover in Mato Grosso state of Brazil. This state has $903,357$km^2^ of extension, being the third largest state of Brazil. It includes three of Brazil's biomes: Amazonia, Cerrado, and Pantanal. It is the most important agricultural frontier of Brazil and it is the largest producer of soybeans, corn, and cotton. The samples contain time series extracted from the MODIS MOD13Q1 product from 2000 to 2016, provided every $16$ days at $250$-meter spatial resolution in the Sinusoidal projection. Based on ground surveys and high resolution imagery, we selected $2,115$ samples of nine classes: "Forest", "Cerrado", "Pasture", "Soybean-fallow", "Fallow-Cotton", "Soybean-Cotton", "Soybean-Corn", "Soybean-Millet", and "Soybean-Sunflower".

```{r}
# data set of samples
data(samples_mt_9classes)
samples_mt_9classes[1:3,]
```

A `sits` tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal location, label assigned to the sample, and coverage from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the "WGS84" ellipsoid. For example, the first sample has been labelled "Pasture", at location ($-55.1852$, $-10.8387$), and is considered valid for the period (2013-09-14, 2014-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers involved in labeling the samples chose to use the agricultural calendar in Brazil, where the spring crop is planted in the months of September and October, and the autumn crop is planted in the months of February and March. For other applications and other countries, the relevant dates will most likely be different from those used in the example. The `time_series` column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. 

```{r}
# print the first time series records of the first sample
samples_mt_9classes$time_series[[1]][1:3,]
```

The `sits` package provides functions for data manipulation and displaying information for `sits` tibbles. For example, `sits_labels()` shows the labels of the sample set and their frequencies.

```{r}
sits_labels(samples_mt_9classes)
```

In many cases, it is useful to relabel the data set. For example, there may be situations when one wants to use a smaller set of labels, since samples in one label on the original set may not be distinguishable from samples with other labels. We then could use `sits_relabel()`, which requires a conversion list (for details, see `?sits_relabel`).

Given that we have used the tibble data format for the metadata and and the embedded time series, one can use the functions from `dplyr`, `tidyr` and `purrr` packages of the `tidyverse` [@Wickham2017] to process the data. For example, the following code uses `sits_select_bands()` to get a subset of the sample data set with two bands (NDVI and EVI) and then uses the `dplyr::filter()` to select the samples labelled either as "Cerrado" or "Pasture". We can then use the `sits_plot()` to display the time series. Given a small number of samples to display, `sits_plot()` tries to group as many spatial locations together. In the following example, the first 15 samples of  "Cerrado" class refer to the same spatial location in consecutive time periods. For this reason, these samples are plotted together.

```{r cerrado-15, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of the first 15 'Cerrado' samples from data set \\texttt{samples_mt_9classes} (different dates for the same point location)."}
# select NDVI band
samples_ndvi.tb <- sits_select_bands_(samples_mt_9classes, 'ndvi')
# select only samples with Cerrado label
samples_cerrado.tb <-
    dplyr::filter(samples_ndvi.tb, label == "Cerrado")
# plot the first 15 samples (different dates for a single point)
sits_plot(samples_cerrado.tb[1:15,])
```

For a large number of samples, where the amount of individual plots would be substantial, the default visualization combines all samples together in a single temporal interval (even if they belong to different years). All samples with the same band and label are aligned to a common time interval. This plot is useful to show the spread of values for the time series of each band. The strong red line in the plot shows the median of the values, while the two orange lines are the first and third interquartile ranges. The documentation of `sits_plot()` has more details about the different ways it can display data.

```{r cerrado-all, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Plot of all Cerrado samples from data set \\texttt{samples_mt_9classes}."}
# plot all cerrado samples together
sits_plot(samples_cerrado.tb)
```

# Importing Data into `sits`

Usually, samples are provided by experts that take *in-loco* field observations or recognize land classes through high resolution images. In any case, we need access to a data source to fetch time series data regarding a spatiotemporal location of interest. The process of importing data samples is discussed in this section.

Package `sits` allows different methods of data input, including: (a) obtain data from a WTSS (Web Series Time Service); (b) obtain data from the SATVEG service developed by EMBRAPA (Brazil's Agriculture Research Agency); (c) read data stored in a time series in the ZOO format [@Zeileis2005]; (d) read a time series from a RasterBrick [@Hijmans2015]. Option (d) will be described in the section were we present raster processing. WTSS is a light-weight service that retrieves time series data for selected locations and periods [@Vinhas2016]. SATVEG service provides NDVI and EVI time series vegetation indices from MODIS image for the whole Brazilian territory [@Embrapa2014]. Function `sits_services()` provides information on the coverages available in the servers.

```{r}
sits_services()
```

After finding out which coverages are available at the different time series services, one may request specific information on each coverage by using `sits_coverage()`. This function lists the contents of the data set, including source, bands, spatial extent and resolution, time range, and temporal resolution. This information is returned as a tibble.

```{r}
# get information about a specific coverage from WTSS
coverage_wtss <- sits_coverage(service = "WTSS-INPE",
                               name    = "MOD13Q1")

coverage_wtss %>% dplyr::select(xmin, xmax, ymin, ymax, timeline)
```

The user can request one or more time series points from a coverage by using `sits_get_data()`. This function provides a general means of access to image time series. In its simplest fashion, the user provides the latitude and longitude of the desired location, the product and coverage names, the bands, and the start date and end date of the time series. If the start and end dates are not provided, it retrieves all the available period. The result is a tibble that can be visualized using `sits_plot()`.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="NDVI and EVI time series fetched from WTSS service."}
# a point in the transition forest to pasture in Northern MT
# obtain a time series from the WTSS server for this point
series.tb <- sits_get_data(longitude = -55.57320, 
                           latitude  = -11.50566,
                           coverage  = coverage_wtss,
                           bands     = c("ndvi", "evi"))
sits_plot(series.tb)
```

A useful case is when a set of labelled samples are available to be used as a training data set. In this case, one usually has trusted observations which are labelled and commonly stored in plain text CSV files. Function `sits_get_data()` can get a CSV file path as an argument. The CSV file must provide, for each time series, its latitude and longitude, the start and end dates, and a label associated to a ground sample. 

After importing the samples time series, it is useful to explore the data to see how it is structured and look for its inter-class separability. For example, we can note in the figure above the variability of 400 time series collected from different years and locations. The scattering behavior is intrinsic to remote sensing data. Atmospheric noise, sun angle, interferences on observations or different equipment specifications, as well as the very nature of the climate-land dynamics can be sources of such variability [@Atkinson2012]. One helpful technique to explore such properties is *cluster analysis*. In the following section we present a clustering technique supported by `sits`.

# Clustering satellite image time series

Cluster analysis has been used for many purposes in satellite image time series literature ranging from unsupervised classification [@Petitjean2011], and pattern detection [@Romani2011]. Here, we are interested in the second use of clustering, using it as a way to improve training data to feed machine learning classification models. In this regard, cluster analysis can assist the identification of structural *time series patterns* and anomalous samples [@Romani2011], [@Chandola2009]. `sits` provides support for the Agglomerative Hierarchical Clustering (AHC). 

Hierarchical clustering is a family of methods that groups elements using a distance function to associate a real value to a pair of elements. From this distance measure, we can compute the dissimilarity between any two elements from a data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. AHC approach is suitable for the purposes of samples data exploration due to its visualization power and ease of use [@Keogh2003]. Moreover, AHC does not require a predefined number of clusters as an initial parameter. This is an important feature in satellite image time series clustering since defining the number of clusters present in a set of multi-attribute time series is not straightforward [@Aghabozorgi2015].

The main result of AHC method is a *dendrogram*. It is the ultrametric relation formed by the successive merges in the hierarchical process that can be represented by a tree. Dendrograms are quite useful to decide the number of clusters to partition the data. It shows the height where each merging happens, which corresponds to the minimum distance between two clusters defined by a *linkage criterion*. The most common linkage criteria are: *single-linkage*, *complete-linkage*, *average-linkage*, and *Ward-linkage*. Complete-linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, which can increase the resulting intracluster data variance. As an alternative, Ward proposes a criteria to minimize the data variance by means of either *sum-of-squares* or *sum-of-squares-error* [@Ward1963]. Ward's intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape [@Hennig2015]. In `sits`, a dendrogram can be generated by `sits_dendrogram()`. The following codes illustrate how to create, visualize, and cut a dendrogram (for details, see `?sits_dendrogram()`).

```{r dendrogram, cache=TRUE, fig.align="center", fig.height=4.1, fig.width=5}
# take a set of patterns for 2 classes
# create a dendrogram object with default clustering parameters
dendro <- sits_dendrogram(cerrado_2classes)
# plot the resulting dendrogram
sits_plot_dendrogram(cerrado_2classes, dendro)
```

After creating a dendrogram, an important question emerges: *where to cut the dendrogram?* The answer depends on what are the purposes of the cluster analysis [@Hennig2015]. If one is interested in an unsupervised classification, it is common to use *internal validity indices*, such as silhouettes [@Rousseeuw1987], to help determine the best number of clusters. However, if one is interested in understanding the structure of a labeled data set, or in the identifying sample anomalies, as we are here, one can use *external validity indices* to assist the semi-supervised procedure in order to achieve the optimal correspondence between clusters and classes partitions. In this regard, we need to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to their known classes. To help this process, `sits` provides `sits_dendro_bestcut()` function that computes an external validity index *Adjusted Rand Index* (ARI) for a series of different number of generated clusters. This function returns the height where the cut of the dendrogram maximizes the index.

```{r}
# search for the best height to cut the dendrogram
sits_dendro_bestcut(cerrado_2classes, dendro)
```

In this example, the height optimizes the ARI and generates $6$ clusters. The ARI considers any pair of distinct samples and computes the following counts:
(a) the number of distinct pairs whose samples have the same label and are in the same cluster;
(b) the number of distinct pairs whose samples have the same label and are in different clusters;
(c) the number of distinct pairs whose samples have different labels and are in the same cluster; and
(d) the number of distinct pairs whose samples have the different labels and are in different clusters.
Here, $a$ and $d$ consist in all agreements, and $b$ and $c$ all disagreements. The ARI is obtained by:

$$
ARI=\frac{a+d-E}{a+d+b+c-E},
$$
where $E$ is the expected agreement, a random chance correction calculated by 
$$
E=(a+b)(b+c)+(c+d)(b+d).
$$ 

Unlike other validity indexes such as Jaccard (${J=a/(a+b+c)}$), Fowlkes-Mallows (${FM=a/(a^2+a(b+c)+bc)^{1/2}}$), and Rand (the same as ARI without the $E$ adjustment) indices, ARI is more appropriate either when the number of clusters is outweighed by the number of labels (and *vice versa*) or when the amount of samples in labels and clusters is imbalanced [@Hubert1985], which is usually the case.

```{r}
# create 6 clusters by cutting the dendrogram at 
# the linkage distance 20.39655
clusters.tb <- sits_cluster(cerrado_2classes, dendro, k = 6)
# show clusters samples frequency
sits_cluster_frequency(clusters.tb)
```

Note in this example that almost all clusters has a predominance of either "Cerrado" or "Pasture" classes with the exception of cluster $3$. The contingency table plotted by `sits_cluster_frequency()` shows how the samples are distributed across the clusters and helps to identify two kinds of confusions. The first is relative to those small amount of samples in clusters dominated by another class (*e.g.* clusters $1$, $2$, $4$, $5$, and $6$), while the second is relative to those samples in non-dominated clusters (*e.g.* cluster $3$). These confusions can be an indication of samples with poor quality, an inadequacy of selected parameters for cluster analysis, or even a natural confusion due to the inherent variability of the land classes.

In an case, it is possible to remove outliers using `sits_cluster_clean()` or `sits_cluster_remove()`. The first one removes all those minority samples that do not reach a minimum percentage close to $0\%$, whereas the second removes an entire cluster if its dominant class does not reach a minimum percentage, close to $100\%$. The example below illustrates the second approach.

```{r}
# clear those samples with a high confusion rate in a cluster 
# (those clusters which majority class does not reach 90% of 
# samples in that cluster)
cleaned.tb <- sits_cluster_remove(clusters.tb, min_perc = 0.9)
# show clusters samples frequency
sits_cluster_frequency(cleaned.tb)
```

Along the process of cluster analysis, it may be a good practice to measure the correspondence between clusters and labels partitions through computation of external validity indices. These metrics help comparing different procedures and assist decision-making. Function `sits_cluster_validity()` provides a way to compute some external validation indices other than ARI. Moreover, these indices capture the samples structure by deriving metrics from its partitions [@Hubert1985].

# Filtering techniques 

Satellite image time series generally is contaminated by atmospheric influence, geolocation error, and directional effects [@Lambin2006]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on an year to year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with  *noisy* and *non-homogeneous* data sets. In this section, we discuss filtering techniques to improve time series data that present missing values or noise.

The literature on satellite image time series have several applications of filtering to correct or smooth vegetation index data. The `sits` have support for Savitzky–Golay (`sits_sgolay()`), Whitaker (`sits_whittaker()`), envelope (`sits_envelope()`) and, the "cloud filter" (`sits_cloud_filter()`) filters. The first two filters are commonly used in the literature, while the remaining two are adapted from other methods and, for our knowledge, its use has not been reported in the literature.

Various somewhat conflicting results have been expressed in relation to the time series filtering techniques for phenology applications. For example, in an investigation of phenological parameter estimation, @Atkinson2012 found that the Whittaker and Fourier transform approaches were preferable to the double logistic and asymmetric Gaussian models. They applied the filters to preprocess MERIS NDVI time series for estimating phenological parameters in India. Comparing the same filters as in the previous work, @Shao2016 found that only Fourier transform and Whittaker techniques improved interclass separability for crop classes and significantly improved overall classification accuracy. The authors used MODIS NDVI time series from the Great Lakes region in North America. @Zhou2016 found that asymmetric Gaussian model outperforms other filters over high latitude boreal biomes, while the Savitzky-Golay model gives the best reconstruction performance in tropical evergreen broadleaf forests. In the remaining biomes, Whittaker gives superior results. The authors compare all previous mentioned filters plus Savitzky-Golay method for noise removal in MODIS NDVI data from sites spread worldwide in different climatological conditions. Many other techniques can be found in applications of satellite image time series such as curve fitting [@Bradley2007], wavelet decomposition [@Sakamoto2005], mean-value iteration, ARMD3-ARMA5, and 4253H [@Hird2009]. Therefore, any comparative analysis of smoothing algorithms depends on the adopted performance measurement.

One of the main uses of time series filtering is to reduce the noise and miss data produced by clouds in tropical areas. The following examples use data produced by the PRODES project [@INPE2017], which detects deforestation in the Brazilian Amazon rain forest through visual interpretation. `sits` provides $617$ samples from a region corresponding to the standard Landsat Path/Row 226/064. This is an area in the East of the Brazilian Pará state. It was chosen because of its huge cloud coverage from November to March, which is a significant factor in degrading time series quality. Its NDVI and EVI time series were extracted from a combination of MOD13Q1 and Landsat8 images (to best visualize the effects of each filter, we selected only NDVI time series).

## Savitzky–Golay filter

The Savitzky-Golay filter works by fitting a successive array of $2n+1$ adjacent data points with a $d$-degree polynomial through linear least squares. The central point $i$ of the window array assumes the value of the interpolated polynomial. An equivalent and much faster solution than this convolution procedure is given by the closed expression
$$
{\hat{x}_{i}=\sum _{j=-n}^{n}C_{j}\,x_{i+j}},
$$
where $\hat{x}$ is the the filtered time series, $C_{j}$ are the Savitzky-Golay smoothing coefficients, and $x$ is the original time series.

The coefficients $C_{j}$ depend uniquely on the polynomial degree ($d$) and the length of the window data points (given by parameter $n$). If ${d=0}$, the coefficients are constants ${C_{j}=1/(2n+1)}$ and the Savitzky-Golay filter will be equivalent to moving average filter. When the time series is equally spaced, the coefficients have analytical solution. According to @Madden1978, for ${d\in{}[2,3]}$ each $C_{j}$ smoothing coefficients can be obtained by
$$
C_{j}=\frac{3(3n^2+3n-1-5j^2)}{(2n+3)(2n+1)(2n-1)}.
$$

In general, the Savitzky-Golay filter produces smoother results for a larger value of $n$ and/or a smaller value of $d$ [@Chen2004]. The optimal value for these two parameters can vary from case to case. For example, @Zhou2016 set ${d=2}$ and ${n=10}$. @Hird2009 tests the filter for ${n\in{}[5,6,7]}$ using quadratic polynomial.

`sits` Savitzky-Golay function  The following example shows the effect of Savitsky-Golay filter on the original time series.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Savitzky-Golay filter applied on a one-year NDVI time series."}
# Take NDVI band of the first sample data set
point.tb <- sits_select_bands(prodes_226_064[1,], ndvi)
# apply Savitzky–Golay filter
point_sg.tb <- sits_sgolay(point.tb)
# plot the series
sits_merge(point_sg.tb, point.tb) %>% sits_plot()
```

## Whittaker filter

The Whittaker smoother attempts to fit a curve that represents the raw data, but is penalized if subsequent points vary too much [@Atzberger2011]. The Whittaker filter is a balancing between the residual to the original data and the "smoothness" of the fitted curve. The residual, as measured by the sum of squares of all $n$ time series points deviations, is given by
$$
RSS=\sum_{i}(x_{i} - \hat{x_{i}})^2,
$$
where $x$ and $\hat{x}$ are the original and the filtered time series vectors, respectively. The smoothness is assumed to be the measure of the the sum of the squares of the third order differences of the time series [@Whittaker1922] which is given by
$$
\begin{split}
S\!S\!D = (\hat{x}_4 - 3\hat{x}_3 + 3\hat{x}_2 - \hat{x}_1)^2 + (\hat{x}_5 - 3\hat{x}_4 + 3\hat{x}_3 - \hat{x}_2)^2 \\ + \ldots + (\hat{x}_n - 3\hat{x}_{n-1} + 3\hat{x}_{n-2} - \hat{x}_{n-3})^2.
\end{split}
$$

The filter is obtained by finding a new time series $\hat{x}$ whose points minimize the expression
$$
RSS+\lambda{}S\!S\!D,
$$ 
where $\lambda{}$, a scalar, works as an "smoothing weight" parameter. The minimization can be obtained by differentiating the expression with respect to $\hat{x}$ and equating it to zero. The solution of the resulting linear system of equations gives the filtered time series which, in matrix form, can be expressed as
$$
\hat{x} = ({\rm I} + \lambda {D}^{\intercal} D)^{-1}x,
$$
where ${\rm I}$ is the identity matrix and 
$$
D = \left[\begin{array}{ccccccc}
1 & -3 & 3 & -1 & 0 & 0 &\cdots \\
0 & 1 & -3 & 3 & -1 & 0 &\cdots \\
0 & 0 & 1 & -3 & 3 & -1 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
\right]
$$
is the third order difference matrix. The Whitakker filter can be a large but sparse optimization problem, as we can note from $D$ matrix. 

Whittaker smoother has been used only recently in satellite image time series investigations. According to @Atzberger2011, the smoother has an advantage over other filtering techniques such as Fourier and wavelets as it does not assume signal periodicity. Moreover, the authors argue that is enables rapid processing of large amounts of data, and handles incomplete time series with missing values. 

In the `sits` package, the Whittaker smoother has two parameters: `lambda` controls the degree of smoothing and `differences` the order of the finite difference penalty. The default values are `lambda = 1` and `differences = 3`. Users should be aware that increasing `lambda` results in much smoother data. When dealing with land use/land cover classes that include both natural vegetation and agriculture, a strong smoothing can reduce the amount of noise in natural vegetation (e.g., forest) time series; however, higher values of `lambda` reduce the information present in agricultural time series, since they reduce the peak values of crop plantations.

The fact that it has only one parameter ($\lambda{}$) facilitates its calibration/comparison process. @Zhou2016 found that $\lambda=15$ gives the best result when compared with $\lambda=2$. Larger values of $\lambda{}$ produces smoother results.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Whittaker smoother filter applied on one-year NDVI time series. The example uses default $\\lambda=1$ parameter."}
# Take EVI band of the first sample data set
point.tb <- sits_select_bands(prodes_226_064[1,], evi)
# apply Whitaker filter
point_whit.tb <- sits_whittaker(point.tb)
# plot the series
sits_merge(point_whit.tb, point.tb) %>% sits_plot()
```

## Envelope filter

This filter can generate a time series corresponding to the superior (inferior) bounding of the input signal. This is accomplished through a convoluting window (odd length) that attributes to the point $i$, in the resulting time series, the maximum (minimum) value of the points in the window. The $i$ point corresponds to the central point of the window. It can be defined as
$$
u_i=\max_{k}{(\{x_{k}:\left|i-k\right|\le{}1\})},
$$
whereas an lower dilation is obtained by
$$
l_i=\min_{k}{(\{x_{k}:\left|i-k\right|\le{}1\})}.
$$
Here, $x$ is the input time series and, $k$ and $i$ are vector indices.

The `sits_envelope()` can combine both maximum and minimum window sequentially. The function can receive a string sequence with `"U"` (for maximization) and `"L"` (for minimization) characters passed to its parameter. A repeated sequence of the same character is equivalent to one operation with a larger window. The sequential operations on the input time series produces the final filtered result that is returned.

The envelope filter can be viewed through mathematical morphology lenses, a very common field in digital image processing [@Haralick1987]. Here the operations of `"U"` and `"L"` corresponds to the *dilation* and *erosion* morphological operators applied to univariate arrays [@Vavra2004]. Furthermore, the compounds operation of *opening* and *closing* can be obtained by `"UL"` and `"LU"`, respectively. This technique has been applied on time series analysis in other fields [@Accardo1997] but, for our knowledge, there is no application in satellite image time series literature.

In the following example we can see an application of `sits_envelope()` function. There, we performs the *opening filtration* and *closing filtration* introduced by @Vavra2004. The correspondent operations sequence are `"ULLULUUL"` and `"LUULULLU"`.

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="Envelope filter applied on one-year NDVI time series. The examples uses two morfological filters: opening filtration (~.OF) and closing filtration (~.CF)."}
# Take the NDVI band of the first sample data set
point.tb <- sits_select_bands(prodes_226_064[1,], ndvi)
# apply envelope filter (remove downward and upward noises)
point_env1.tb <- 
    sits_envelope(point.tb,
                  "ULLULUUL",
                  bands_suffix = "OF")
point_env2.tb <- 
    sits_envelope(point.tb,
                  "LUULULLU",
                  bands_suffix = "CF")
# plot the series
sits_merge(point_env1.tb, point_env2.tb) %>%
    sits_merge(point.tb) %>%
    sits_plot()
```

## ARIMA filter for cloud removal

The cloud filter makes use of the well known autoregressive integrated moving average (ARIMA) model. The algorithm looks to the first order difference time series for points where the value is above a certain threshold. This procedure selects only those points with large variations in the original time series, probably associated with noise. Finally, these points are replaced by the ARIMA correspondent values.

The parameters of the ARIMA model can be set by the user. Please see `arima` for the detailed description of parameters $p$, $d$, and $q$.

```{r, fig.align="center", fig.height=3.1, fig.width=5}
# Take the NDVI band of the first sample data set
point.tb <- sits_select_bands(prodes_226_064[1,], ndvi)
# apply ARIMA filter
point_cf.tb <- sits_ndvi_arima_filter(point.tb, apply_whit = FALSE)
# plot the series
sits_merge(point_cf.tb, point.tb) %>% sits_plot()
```
# Evaluating samples with self-organizing maps

Conventional approaches use  only supervised classification to map LUCC, however to achieve  maps more accurated, aproaches as pre processing and pos processing must be required. sits allows to exploit a clustering method as pre-processing step using specifically self-organizing maps (SOM) to assess land cover samples of vegetation indexes time series. Independent of classifer, these techniques are suitable tools for assisting users to select representative land cover change samples from vegetation indexes time series through metrics hat indicate the sample quality and separability. 

Using SOM, we can improve the performance of satellite image time series analysis for producing LUCC maps. SOM is an unsupervised neural network algorithm used to map high dimensional space onto low-dimensional space. The structure of SOM is composed by input and output layers. In the input layer contains all time series to be evaluated, also called training data. The output layer consist of a 2-D grid of neurons. Each neuron has a vector of weights representing a pattern of the input data. These neurons are organized into order in which similar pattern are closer to each other in the grid [@Kohonen2001].

To train the network, the neurons are initialized randomly. The vector of weights of neuron $j$,  $w_j=[w_{j1},\ldots,w_{jn} ]$,  must have the same dimension of a input vector of time series samples $x(t)=[x(t)_1,\ldots,x(t)_n ]$. At each training step $t$, a time series sample $x(t)$ is presented to the network to find the neuron whose weight vector has the smaller distance between them. Distance metrics are utilized to compute distance $D_{j}$ between input vector $x(t)$ and each weight vector of neuron, $w_j$ for each neuron j in the output layer. The most part of literature uses Euclidean distance as metric.

$$
D_{j}=\sum_{i=1}^{n}{\sqrt{(x(t)_i{-}w_{ji})^{2}}}.
$$

The neuron that contains the shortest distance is declared the winner neuron, defined here as ${b}$, also know as Best Matching Unit (BMU):


$$
 d_{b}= min \left\{D_1,\ldots, D_J \right\}.
$$

The BMU and its neighborhood must be updated. The weights are adjusted to increase the similarity with input vector.
The equation for updating the weight vector is given by:

$$
 w_{j}(t{+}1)=  w_{j}(t){+}\alpha(t)* h_{b,j}(t) [x(t)_i{-}w_{j}(t)],
$$
where $\alpha(t)$ is the learning rate, and it must be set as $0<\alpha(t)<1$ and $h_{b,j}(t)$ is a neighbourhood function.

In sits, the package kohonen (referencia) is used as basis to run the SOM, after the training step, to separate the samples in groups is necessary to label the neurons. A cluster can be one neuron or a set of neurons that belongs to same category. In this step, a neuron is named using majority vote technique through of original label given from sample data set. As a result of this, a sample that was associated to a neuron receives the same label of it. In some cases, no samples is associated to a neuron, then, it receives the label "Noclass".

```{r}
data("samples_mt_9classes", package = "sits")

```
```{r}
data.tb <- samples_mt_9classes
```


```{r}
# clustering time series using SOM
kohonen_cluster <-
    sits::sits_kohonen(
        data.tb,
        bands = c("evi", "ndvi"),
        grid_xdim = 25,
        grid_ydim = 25,
        rlen = 100,
        distance = "euclidean",
        neighbourhood.fct = "gaussian",
        mode = "online"

    )
# return a tibble with all samples and which cluster it belongs
samples_cluster <- kohonen_cluster$info_samples
samples_cluster [1:10,]
```

To verify the quality of clusters generated by SOM, the confusion matrix can be accessed. From the confusion matrix percentage of mixture within a cluster is shown. 

```{r}

# Calculate the percentage of confusion within a cluster 
confusion_by_cluster <- sits_evaluate_cluster(kohonen_cluster$info_samples)
confusion_by_cluster$mixture_cluster[1:5,]

```

```{r}
# confusion matrix
confusion_matrix <- confusion_by_cluster$confusion_matrix

sits_plot_cluster_info(confusion_by_cluster, "Confusion by Cluster")
```


Besides that, to evaluate the samples with greater reliability, the process of clustering and labelling can be executed T times. 
The process of clustering samples with SOM can be performed several times $t$ to evaluate the reliability of each sample. During this procedure, a historical table is created in order to store informations about sample and the cluster which it was associated at step $t$ %.
The historical table contains for each sample the fields  identifier, the original label of a sample, given from sample dataset, the identifier and  label of neuron whose the sample was associated, and finally, the number of iteration in the clustering step.

From historical table, metrics can be extracted to compute the probabilities of a sample belongs to a cluster. The contingency table for each sample is used to summarize the news labels associated to it, thus a new table summarized is created containing the fields the number of time that a sample $s_i$ was associated to a new label and the probabilities in percentage of a sample belongs to a cluster, identifier and original label of sample. From this metrics, the user can make decisions like to remove samples of dataset in order to improve the classification. 


```{r}
# run clustering 10 times to evaluate samples
evaluate_samples <-
    sits::sits_evaluate_samples(
        data.tb,
        grid_xdim = 25,
        grid_ydim = 25,
        rlen = 10,
        distance = "euclidean",
        mode = "online",
        iteration = 10
    )

#


```

```{r}
metrics_samples.tb <- evaluate_samples$metrics_by_samples
metrics_samples.tb[1:10,]
```

# Machine learning classification for land use and land cover using satelite image time series

The main advantage using satellite image time series in land use studies is that the time series is methodologically consistent with the very nature of the land covers. Using this kind of data allows focusing on land changes through time. 

The `sits` package provides functionality to explore the full depth of satellite image time series data. It treat time series as a feature vector. To be consistent, the procedure aligns all time series from different years by its time proximity considering an given cropping schedule. Once aligned, the feature vector is formed by all pixel "bands". The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars (the training data) to infer classes of a larger data set. `sits` has support for a variety of machine learning techniques: linear discriminant analysis, quadratic discriminant analysis, multinomial logistic regression, . In the following sections we discuss on support vector machine and random forest machine learning techniques with more detail.

## Support Vector Machine

In its base model, the Support Vector Machine (SVM) is a binary classifier that finds a linear boundary in some feature space that not only divides the points of two classes but maximizes the distance between the boundary and the observations. A boundary is a ${(p-1)}$-dimensional *hyperplane* that defines two complementary subspaces in a $p$-dimensional feature space.

If the sample observations are linearly separable in the feature space, the hyperplane has a perfect classification property. However, this is hardly what one may expect in a typical satellite image time series scenario. In this regard, the hyperplane optimization problem has a *softner term* that allows some observations to be closer to the margin, or even in the wrong side of its boundary. This relaxation increases the robustness of SVM as it decreases the influence of individual observations on the hyperplane determination. 

Moreover, the solution for the hyperplane coefficients depends only on those samples that violates the maximum margin criteria, the so-called *support vectors*. All other points far away from the hyperplane does not exert any influence on the hyperplane coefficients which let SVM less sensitive to outliers.

Hyperplanes are linear ${(p-1)}$-dimensional boundaries and define linear partitions in the feature space. However, one can enlarge the input attribute space by transforming it into a higher degree feature space. In this manner, the new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundaries in the original attribute space. As that enlargement can be computationally expensive, SVM makes use of *kernels* functions to overcome such limitation. The use of kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space an hence can improve training-class separation.

In `sits`, SVM is the default machine learning model. As a wrapper of `e1071` R package that uses the `LIBSVM` implementation [@Chang2011], `sits` adopts the *one-against-one* method for multiclass classification. For a $q$ class problem, this method creates ${q(q-1)/2}$ SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme. The following example illustrate how to classify an individual sample (in this case a series of 16 one-year NDVI time series from the same location). We used the NDVI time series from Mato Grosso Brazilian state as a training data set.

```{r, fig.align="center", fig.height=3.4, fig.width=5.5, fig.cap="SVM classification of a $16$ years time series. The location (latitude, longitude) shown at the top of the graph is in geographic coordinate system (WGS84 {\\it datum})."}
# Retrieve the set of samples (provided by EMBRAPA) from the 
# Mato Grosso region for train the SVM model
data(samples_mt_ndvi)
# train a machine learning model using SVM
svm_model <- sits_train(samples_mt_ndvi,
                        sits_svm(kernel = "radial",
                                 cost = 10))
# get a point to be classified
data(point_ndvi)
# Classify using SVM model
class.tb <- sits_classify(point_ndvi, svm_model)
sits_plot(class.tb)
```

## Random forest

Random forest uses the idea *decision tree* as its base model. It combines many decision trees via *bootstrap* procedure and *stochastic feature selection*, developing a population of somewhat uncorrelated base models. The final classification model is obtained by a majority voting schema. This procedure decreases the classification variance, improving prediction of individual decision trees. 

Random forest training process is essentially nondeterministic. It starts by growing trees through repeatedly random sampling-with-replacement the observations set. At each growing tree, the random forest considers only a fraction of the original attributes to decide where to split a node, according to a *purity criterion*. This decreases the correlation among trees and improves the prediction performance. The most used impurity criterion are *Gini*, *cross-entropy*, and *misclassification error*. The splitting process continues until the tree reaches some given minimum nodes size or a minimum impurity index value.

Random forest provides better performances than *bagged trees*, a similar procedure that does not implement stochastic feature selection (*i.e.* when $m=p$). Bagged trees suffer from high correlation among trees introduced by an eventual presence of strong predictors that tends to be chosen as the splitting criterion [@James2013]. However, the random forest classification performance can vary according to the tunning of the model parameters. The main random forest parameters are the number of attributes sampled as candidates at each split, the number of decision trees to grow, the minimum node size, and the sample fraction to be drawn at each bootstrap iteration. 

<!--
% Lower values of $m$ tends to grow taller and uncorrelated decision trees, but decreases the model classification accuracy. The parameter $b$, the number of trees composing the random forest ensemble, is connected with the variance reduction of the model. High values of $b$ can reduce the variance of the model up to a certain level (dependent on the observations data set) but reduces the model performance. Lower values of $N_{\!m\!i\!n}$ parameter can introduce model overfitting but may increase the classification accuracy. Finally, lower values of $\lambda$ tends to increase the model variance but can underrepresent the sample universe reducing its accuracy.
-->

```{r, fig.align="center", fig.height=3.4, fig.width=5.5, fig.cap="Random forest classification of a $16$ years time series. The location (latitude, longitude) shown at the top of the graph are in geographic coordinate system  (WGS84 {\\it datum})."}
# Retrieve the set of samples (provided by EMBRAPA) from the 
# Mato Grosso region for train the Random Forest model.
data(samples_mt_ndvi)
# train a machine learning model using random forest
rfor_model <- sits_train(samples_mt_ndvi, sits_rfor(num.trees = 1000))
# get a point to be classified
data(point_ndvi)
# Classify using Random Forest model
class.tb <- sits_classify(point_ndvi, rfor_model)
sits_plot(class.tb)
```

## Deep learning methods 

In `sits`, the interface to deep learning models is done using the `keras` package [@Chollet2018]. This package implements different deep learning techniques. Currently, `sits` provides access to deep feedforward networks. Also called feedforward neural networks, or multilayer perceptrons (MLPs), these are the quintessential deep learning models. The goal of a feedforward network is to approximate some function $f$. For example, for a classifier $y =f(x)$ maps an input $x$ to a category $y$. A feedforward network defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation. These models are called feedforward because information flows through the function being evaluated from $x$, through the intermediate computations used to define $f$, and finally to the output $y$. There are no feedback connections in which outputs of the model are fed back into itself [@Goodfellow2016].

Specifying a MLP requires some work on customization, which requires some amount of trial-and-error by the user, since there is no proven model for classification of satellite image time series. The most important decision is the number of layers in the model. Initial tests indicate that 3 to 5 layers are enough to produce good results. The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.

Three other important parameters for an MLP are: (a) the activation function; (b) the optimization method; (c) the dropout rate. The activation function  the activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [@Goodfellow2016], we recommend the use of the "relu" and "elu" functions. The optimization method is a crucial choice, and the most common choices are gradient descent algorithm. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [@Ruder2016]. Based on experience with image time series, we recommend that users start by using the default method provided by `sits`, which is the `optimizer_adam` method. Please refer to the `keras` package documentation for more information.

The dropout rates have a huge impact on the performance of deep learning classifiers. Dropout is a technique for randomly  dropping  units  from  the  neural network during training [@Srivastava2014]. By randomly discarding some neurons, dropout reduces overfitting. It is a counter-intuitive idea that works well. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some of these neurons may seem a waste of resources. In fact, as experience has shown [@Goodfellow2016], this procedures prevents an early convergence of the optimization to a local minimum. Thus, in practice, dropout rates between 50% and 20% are recommended for each layer. 

In the following example, we classify the same data set using a simple example of the `deep learning` method, for fast processing: (a) Two layers with 512 neurons each; (b) Using the 'elu' activation function and 'optimizer_adam'; (c) dropout rates of 40% and 30% for the layers. 

```{r, fig.align="center", fig.height=3.4, fig.width=5.5, fig.cap="Deep learning classification of a $16$ year time series. The location (latitude, longitude) shown at the top of the graph are in geographic coordinate system  (WGS84 {\\it datum})."}
# Retrieve the set of samples (provided by EMBRAPA) from the 
# Mato Grosso region for train the  model.
data(samples_mt_ndvi)
# train a machine learning model using deep learning
train_dl <- sits_deeplearning(
    units            = c(512, 512),
    activation       = "elu",
    dropout_rates    = c(0.40, 0.30),
    optimizer        = keras::optimizer_adam(lr = 0.001),
    epochs           = 50,
    batch_size       = 128,
    validation_split = 0.2)

dl_model <- sits_train(samples_mt_ndvi, train_dl)
# get a point to be classified
data(point_ndvi)
class.tb <- sits_classify(point_ndvi, dl_model)
sits_plot(class.tb)
```

# Validation techniques

Validation is a process undertaken on models to estimate some error associated with them, and hence has been used widely in different scientific disciplines. Here, we are interested in estimating the prediction error associated to some model. For this purpose, we concentrate on the *cross-validation* approach, probably the most used validation technique [@Hastie2009].

To be sure, cross-validation estimates the expected prediction error. It uses part of the available samples to fit the classification model, and a different part to test it. The so-called *k-fold* validation, we split the data into $k$ partitions with approximately the same size and proceed by fitting the model and testing it $k$ times. At each step, we take one distinct partition for test and the remaining ${k-1}$ for training the model, and calculate its prediction error for classifying the test partition. A simple average gives us an estimation of the expected prediction error. 

A natural question that arises is: *how good is this estimation?* According to @Hastie2009, there is a bias-variance trade-off in choice of $k$. If $k$ is set to the number of samples, we obtain the so-called *leave-one-out* validation, the estimator gives a low bias for the true expected error, but produces a high variance expectation. This can be computational expensive as it requires the same number of fitting process as the number of samples. On the other hand, if we choose ${k=2}$, we get a high biased expected prediction error estimation that overestimates the true prediction error, but has a low variance. The recommended choices of $k$ are $5$ or $10$ [@Hastie2009], which somewhat overestimates the true prediction error.

`sits_kfold_validate()` gives support the k-fold validation in `sits`. The following code gives an example on how to proceed a k-fold cross-validation in the package. It perform a five-fold validation using SVM classification model as a default classifier. We can see in the output text the corresponding confusion matrix and the accuracy statistics (overall and by class).

```{r}
# read a set of samples
data(cerrado_2classes)

# perform a five fold validation with the 
# SVM machine learning method using default parameters
prediction.mx <- 
    sits_kfold_validate(cerrado_2classes, 
                        folds = 5,
                        ml_method = sits_svm())
# prints the output confusion matrix and statistics 
sits_conf_matrix(prediction.mx)
```

# Raster classification

The continuous observation of the Earth surface provided by orbital sensors is unprecedented in history. Just for the sake of illustration, a unique tile from MOD13Q1 product, a square of $4800$ pixels provided every 16 days since February 2000 takes around $18$GB of uncompressed data to store only one band or vegetation index. This data deluge puts the field into a big data era and imposes challenges to design and build technologies that allow the Earth observation community to analyse those data sets [@Gilberto2017]. `sits` implements an "out of the box" classification scheme based on *raster bricks* where stacked images have spatial and time dimensions.

Our tested approach (see example illustrated bellow) is `GeoTIFF` bricks, where the temporal dimension is stored in the `GeoTIFF` bands. For a multiband, multitemporal image, all time instances of each band should be stored together in a single `GeoTIFF` file. Different bands for the same images should be stored The classification algorithm implemented in `sits_classify_raster()` allows one to choose how many process will run the task in parallel, and also the size of each data chunk to be consumed at each iteration. This strategy enables `sits` to work on average desktop computers without depleting all computational resources. The code bellow illustrates how to classify a small raster brick image that accompany the package.

```{r, fig.align="center", fig.height=3.4, fig.width=4.1, fig.cap="Image (${11\\times14}$ pixels) classified using SVM. The image coordinates ({\\it meters}) shown at vertical and horizontal axis are in MODIS sinusoidal projection."}
# Retrieve the set of samples for the Mato Grosso region 
data(samples_mt_ndvi)

# build a machine learning model for this area
svm_model <- sits_train(samples_mt_ndvi, sits_svm())

# read a raster file and put it into a vector
file <- system.file("extdata/raster/mod13q1/sinop-crop-ndvi.tif", 
                    package = "sits")

# define the timeline
data("timeline_modis_392")

# create a raster metadata file based on the 
# information about the files
raster.tb <- 
    sits_coverage(service  = "RASTER",
                  name     = "Sinop-crop",
                  timeline = timeline_modis_392,
                  bands    = "ndvi",
                  files    = file)

# classify the raster file
raster_class.tb <- 
    sits_classify_raster(file = paste0(tempdir(), "/raster-class"), 
                         raster.tb, 
                         ml_model = svm_model, 
                         memsize = 2, 
                         multicores = 1)

# plot the first raster object with a selected color pallete
# make a title, define the colors and the labels)
sits_plot_raster(raster_class.tb[1,], title = "SINOP-MT - 2000/2001")
```

The classified files can also be visualized with applications such as QGIS. Note that we create two coverage `tibbles` with metadata information, one for the input data and other for the output.  To create the input data, we need a `timeline` that matches the input images of the raster brick. Once created, the coverage can be used either to retrieve time series data from the raster bricks using `sits_get_data()` or to do the raster classification by calling the function `sits_classify_raster()`. The machine learning model and the training data to be used are passed by the arguments `sits_svm()` (default) and `samples_mt_ndvi` parameters. The classification result is stored as a set of files beginning with `file` prefix.

```{r, include=FALSE}
# remove all files
file.remove(unlist(raster_class.tb$files))
```
# Processing large raster data files

One of the challenges in land use classification is being able to process large data files. To reduce processing time, it is necessary to adjust `sits_classify_raster()` according to the capabilities of the server. The package tries to keep memory use to a minimum, performing garbage collection to free memory as often as possible. Nevertheless, there is an inevitable trade-off between computing time, memory use, and I/O operations. The best trade-off has to be determined by the user, considering issues such disk read speed, number of cores in the server, and CPU performance.

The first parameter is `memsize`. It controls the size of the main memory (in GBytes) to be used for classification. The user must specify how much free memory will be available. The second factor controlling performance of raster classification is `multicores`. Once a block of data is read from disk into main memory, it is split into different cores, as specified by the user. In general, the more cores are assigned to classification, the faster the result will be. However, there are overheads in switching time, especially when the
server has other processes running.

Based on current experience, the classification of a MODIS tile (4800 x 4800) with four bands and 400 time instances, covering 15 years of data, using SVM with a training data set of about 10,000 samples, takes about 24 hours using 20 cores and a memory size of 60 GB, in a server with 2.4GHz Xeon CPU and 96 GB of memory to produce the yearly classification maps.

# Smoothing of raster data after classification




# Final remarks

Current approaches to image time series analysis still use limited number of attributes. A common approach is deriving a small set of phenological parameters from vegetation indices, like beginning, peak, and length of growing season [@Brown2013], [@Kastens2017], [@Estel2015], [@Pelletier2016]. These phenological parameters are then fed in specialized classifiers such as TIMESAT [@Jonsson2004]. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces with big training data sets [@James2013].

Package `sits` uses the full depth of satellite image time series to create larger dimensional spaces. We tested different methods of extracting attributes from time series data, including those reported by @Pelletier2016 and @Kastens2017. Our conclusion is that part of the information in raw time series is lost after filtering. Thus, the method we developed uses all the data available in the time series samples. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. Our experiments found out that modern statistical models such as support vector machines, and random forests perform better in high-dimensional spaces than in lower dimensional ones. 

# Acknowledgements

The authors would like to thank all the researchers that provided data samples used in the examples: Alexandre Coutinho, Julio Esquerdo and Joao Antunes (Brazilian Agricultural Research Agency, Brazil) who provided ground samples for "soybean-fallow", "fallow-cotton", "soybean-cotton", "soybean-corn", "soybean-millet", "soybean-sunflower", and "pasture" classes; Rodrigo Bergotti (National Institute for Space Research, Brazil) who provided samples for "cerrado" and "forest" classes; and Damien Arvor (Rennes University, France) who provided ground samples for "soybean-fallow" class. 

This work was partially funded by the São Paulo Research Foundation (FAPESP) through eScience Program grant 2014/08398-6. We thank the Coordination for the Improvement of Higher Education Personnel (CAPES) and National Council for Scientific and Technological Development (CNPq) grants 312151/2014-4 (GC) and 140684/2016-6 (RS). We thank Ricardo Cartaxo, Lúbia Vinhas, and Karine Ferreira who provided insight and expertise to support this paper.

This work has also been supported  by the International Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety under Grant Agreement 17-III-084-Global-A-RESTORE+ (``RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil''). 

<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->
